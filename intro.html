<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Revisiting Shapley Values: A Causal Perspective - 1&nbsp; Introduction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./framework.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Revisiting Shapley Values: A Causal Perspective</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./framework.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Evaluation Framework</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Background</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./explanations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Shapley Explanations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discussion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Discussion</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Conclusion</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">Appendix</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Machine learning models are increasingly being used to replace or augment human judgment. The decisions that these models support have important ramifications, especially in high-stakes medical and financial settings. As a result, there is a growing demand for model explanations that address why a particular prediction was made. While some models are inherently interpretable, others are black-boxes and do not lend themselves easily to explanation. A model may be black box either because it is too complicated to understand, or because access to the model is limited. For example, a model may functionally be a black box if only the model’s predictions are made available for the purposes of explanation. The ubiquitous use of such black-box models to make decisions, has fueled the development of the field of explainable AI (XAI), which encompasses a broad set of methods that can explain a model’s decisions.</p>
<p>Methods derived from the Shapley value are one of the most common ways to generate post-hoc explanations of black-box models. The Shapley value is a solution concept from cooperative game theory that quantifies how to fairly distribute the payout received by a group to each player based on their contribution <span class="citation" data-cites="shapley_value_1953">Shapley (<a href="references.html#ref-shapley_value_1953" role="doc-biblioref">1953</a>)</span>. Most Shapley methods treat a model’s features as the players in the game and some aspect of the model, such as its prediction, loss, or variance explained, as the payout. To compute the Shapley value, the explainer specifies a value function, which indicates the payout (e.g.&nbsp;the model’s prediction) that is received for all possible subsets of features. Since most machine learning models cannot handle missing inputs, the value function is responsible for substituting a value for any feature that is not part of the subset. In essence, it must simulate what happens when features are removed from the model (see <span class="citation" data-cites="janzing_feature_2019">Janzing, Minorics, and Blöbaum (<a href="references.html#ref-janzing_feature_2019" role="doc-biblioref">2019</a>)</span>, <span class="citation" data-cites="merrick_explanation_2020">Merrick and Taly (<a href="references.html#ref-merrick_explanation_2020" role="doc-biblioref">2020</a>)</span>, and <span class="citation" data-cites="covert_explaining_2020">Covert (<a href="references.html#ref-covert_explaining_2020" role="doc-biblioref">2020</a>)</span>). Numerous options have been proposed: train different models for each subset of features, use a fixed value (e.g.&nbsp;zero), use a value from a reference data point, or use the expected value over a collection of reference points. Each approach yields different results, all of which are, strictly speaking, Shapley values. The decision about how to specify the value function also impacts the interpretation of the resulting values. These differences are not only quantitatively significant, but also take on real-world significance when they are used to explain a model’s decision. For example, Shapley explanations have been proposed as a tool for providing recourse recommendations to subjects of automated decisions, for ensuring algorithmic fairness, and other applications. However, it is quite likely that different Shapley methods will lead to explanations, which lend themselves to different conclusions. It is therefore vitally important for practitioners to make an informed decision about which method is appropriate for a given situation.</p>
<p>Shapley-based methods are used to provide different levels of explanation. Model level explanations treat the model as an input-output system, while world-level explanations account for relationships that exist in the world. Failure to clearly distinguish between different levels of explanation has fueled two ongoing debates in the literature: the choice over which value function is appropriate and whether or not features that indirectly influence the model should have non-zero Shapley values. Recent work resolves these debates by suggesting that what is correct is context-dependent (<span class="citation" data-cites="chen_true_2020">Chen et al. (<a href="references.html#ref-chen_true_2020" role="doc-biblioref">2020</a>)</span>, <span class="citation" data-cites="covert_explaining_2020">Covert (<a href="references.html#ref-covert_explaining_2020" role="doc-biblioref">2020</a>)</span>). While we fundamentally agree, we arrive at this conclusion in a more principled way by considering these debates from a causal perspective that is grounded in a model explanation evaluation framework.</p>
<p>In this work, we propose a human-centric framework for evaluating model explanations and review existing Shapley methods in light of this framework. Our primary goal is to equip practitioners with the information necessary to make informed decisions when generating model explanations. Selecting an appropriate method is challenging because it requires evaluating the quality of an explanation. Much of the work in XAI either ignores this problem entirely, or leverages an implicit operational definition of correctness. In the following section, we propose a human-centric framework for assessing the quality of an explanation based on the premise that correct explanations are ones that align with an explainee’s objectives. Next, we provide an introduction to the formalisms used in causal inference as causal reasoning is essential to evaluating explanations. We then provide a brief introduction to Shapley values and how they are used to generate model explanations. With all of this context, we then review existing Shapley methods from a causal perspective and point out the key considerations that practitioners should keep in mind based on our proposed evaluation framework. Combining the prior sections, we then provide practical guidance for selecting an appropriate explanation method. In the final section, we take a step back and consider how a causal perspective changes our understanding of the Shapley explanation literature and its implications for future work on Shapley-based model explanations.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-chen_true_2020" class="csl-entry" role="doc-biblioentry">
Chen, Hugh, Joseph D. Janizek, Scott Lundberg, and Su-In Lee. 2020. <span>“True to the <span>Model</span> or <span>True</span> to the <span>Data</span>?”</span> <em>arXiv:2006.16234 [Cs, Stat]</em>, June. <a href="http://arxiv.org/abs/2006.16234">http://arxiv.org/abs/2006.16234</a>.
</div>
<div id="ref-covert_explaining_2020" class="csl-entry" role="doc-biblioentry">
Covert, Ian C. 2020. <span>“Explaining by <span>Removing</span>: <span>A</span> <span>Uniﬁed</span> <span>Framework</span> for <span>Model</span> <span>Explanation</span>.”</span> <em>arXiv:2011.14878 [Cs]</em>, 90.
</div>
<div id="ref-janzing_feature_2019" class="csl-entry" role="doc-biblioentry">
Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. 2019. <span>“Feature Relevance Quantification in Explainable <span>AI</span>: <span>A</span> Causal Problem.”</span> <em>arXiv:1910.13413 [Cs, Stat]</em>, November. <a href="http://arxiv.org/abs/1910.13413">http://arxiv.org/abs/1910.13413</a>.
</div>
<div id="ref-merrick_explanation_2020" class="csl-entry" role="doc-biblioentry">
Merrick, Luke, and Ankur Taly. 2020. <span>“The <span>Explanation</span> <span>Game</span>: <span>Explaining</span> <span>Machine</span> <span>Learning</span> <span>Models</span> <span>Using</span> <span>Shapley</span> <span>Values</span>.”</span> <em>arXiv:1909.08128 [Cs, Stat]</em>, June. <a href="http://arxiv.org/abs/1909.08128">http://arxiv.org/abs/1909.08128</a>.
</div>
<div id="ref-shapley_value_1953" class="csl-entry" role="doc-biblioentry">
Shapley, L. 1953. <span>“A <span>Value</span> for n-<span>Person</span> <span>Games</span>.”</span> In <em>Contributions to the <span>Theory</span> of <span>Games</span></em>, 2:307–17. Princeton, NJ: Princeton University Press.
</div>
</div>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Welcome</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./framework.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Evaluation Framework</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>