[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Revisiting Shapley Values: A Causal Perspective",
    "section": "",
    "text": "Welcome\nThe following is a version of my master’s thesis submitted to the faculties of the University of Pennsylvania in May 2022. As the title suggests, this work is effectively a literature review, but done through the lens of causality. It is not meant to be a systematic review and is therefore certainly missing many relevant works. It is also not condensed enough to be a conference paper, nor does it provide any novel theoretical contributions. This left it in a precarious spot in terms of outlets for publication. Now that I’ve had some time away from the project, I’ve decided to simply make it easily accessible in the hopes of getting feedback. In that vein, if you have any feedback about this work, please drop me a note via email (zachduey@gmail.com) or open a PR.\n\n\nAcknowledgements\nI am grateful to Lyle Ungar for serving as my supervisor and sheparding this project from its origins as independent study in the Fall of 2021. I would also like to thank Osbert Bastani for co-supervising this work. This work has benefited immensely from collaboration with Tony Liu. I would not have known where to begin without his guidance and would not have gone as deep into the subject without our sessions closely reviewoing many of the papers that form the basis for this work. Finally, I am forver indebted to my wife, Meghan Angelos, who graciously shouldered the burden of being a solo parent while I was pushing to complete this project.\n\n\nAbstract\nShapley values, a concept from cooperative game theory, are used to provide explanations of machine learning models. However, there are many ways to formulate the underlying game, leading to a multitude of Shapley-based methods. The key differentiator between these methods is the value function. Different choices yield substantially different values, which have different interpretations when used as explanations. These differences force practitioners to – oftentimes implicitly – decide which one is correct. To make this decision in an explicit and informed manner requires defining what constitutes a correct explanation. In this work, we revisit existing Shapley explanation methods using a human-centric framework for assessing model explanations. Our framework is grounded in causal reasoning and built on the premise that correct explanations should align with an explainee’s objectives. Selecting an explanation method requires understanding the explainee’s desired level of explanation, whether the explanation desired is based on an associative, interventional, or counterfactual question, and the degree of causal information that the explainee is able to provide. This approach not only surfaces the connection between two ongoing debates in the Shapley explanation literature – whether explanations should be “true to the model” or “true to the data’’ and whether functionally-irrelevant features should receive non-zero attributions – but also provides a theoretically-grounded resolution. Moreover, our framework illuminates causality as a conceptual bridge between Shapley explanations and other explanation-generating methods. The connection between causality and explanation is not new, but has implications both for future work on Shapley explanations, and other research areas within explainable AI.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Machine learning models are increasingly being used to replace or augment human judgment. The decisions that these models support have important ramifications, especially in high-stakes medical and financial settings. As a result, there is a growing demand for model explanations that address why a particular prediction was made. While some models are inherently interpretable, others are black-boxes and do not lend themselves easily to explanation. A model may be black box either because it is too complicated to understand, or because access to the model is limited. For example, a model may functionally be a black box if only the model’s predictions are made available for the purposes of explanation. The ubiquitous use of such black-box models to make decisions, has fueled the development of the field of explainable AI (XAI), which encompasses a broad set of methods that can explain a model’s decisions.\nMethods derived from the Shapley value are one of the most common ways to generate post-hoc explanations of black-box models. The Shapley value is a solution concept from cooperative game theory that quantifies how to fairly distribute the payout received by a group to each player based on their contribution Shapley (1953). Most Shapley methods treat a model’s features as the players in the game and some aspect of the model, such as its prediction, loss, or variance explained, as the payout. To compute the Shapley value, the explainer specifies a value function, which indicates the payout (e.g. the model’s prediction) that is received for all possible subsets of features. Since most machine learning models cannot handle missing inputs, the value function is responsible for substituting a value for any feature that is not part of the subset. In essence, it must simulate what happens when features are removed from the model (see Janzing, Minorics, and Blöbaum (2019), Merrick and Taly (2020), and Covert (2020)). Numerous options have been proposed: train different models for each subset of features, use a fixed value (e.g. zero), use a value from a reference data point, or use the expected value over a collection of reference points. Each approach yields different results, all of which are, strictly speaking, Shapley values. The decision about how to specify the value function also impacts the interpretation of the resulting values. These differences are not only quantitatively significant, but also take on real-world significance when they are used to explain a model’s decision. For example, Shapley explanations have been proposed as a tool for providing recourse recommendations to subjects of automated decisions, for ensuring algorithmic fairness, and other applications. However, it is quite likely that different Shapley methods will lead to explanations, which lend themselves to different conclusions. It is therefore vitally important for practitioners to make an informed decision about which method is appropriate for a given situation.\nShapley-based methods are used to provide different levels of explanation. Model level explanations treat the model as an input-output system, while world-level explanations account for relationships that exist in the world. Failure to clearly distinguish between different levels of explanation has fueled two ongoing debates in the literature: the choice over which value function is appropriate and whether or not features that indirectly influence the model should have non-zero Shapley values. Recent work resolves these debates by suggesting that what is correct is context-dependent (Chen et al. (2020), Covert (2020)). While we fundamentally agree, we arrive at this conclusion in a more principled way by considering these debates from a causal perspective that is grounded in a model explanation evaluation framework.\nIn this work, we propose a human-centric framework for evaluating model explanations and review existing Shapley methods in light of this framework. Our primary goal is to equip practitioners with the information necessary to make informed decisions when generating model explanations. Selecting an appropriate method is challenging because it requires evaluating the quality of an explanation. Much of the work in XAI either ignores this problem entirely, or leverages an implicit operational definition of correctness. In the following section, we propose a human-centric framework for assessing the quality of an explanation based on the premise that correct explanations are ones that align with an explainee’s objectives. Next, we provide an introduction to the formalisms used in causal inference as causal reasoning is essential to evaluating explanations. We then provide a brief introduction to Shapley values and how they are used to generate model explanations. With all of this context, we then review existing Shapley methods from a causal perspective and point out the key considerations that practitioners should keep in mind based on our proposed evaluation framework. Combining the prior sections, we then provide practical guidance for selecting an appropriate explanation method. In the final section, we take a step back and consider how a causal perspective changes our understanding of the Shapley explanation literature and its implications for future work on Shapley-based model explanations.\n\n\n\n\nChen, Hugh, Joseph D. Janizek, Scott Lundberg, and Su-In Lee. 2020. “True to the Model or True to the Data?” arXiv:2006.16234 [Cs, Stat], June. http://arxiv.org/abs/2006.16234.\n\n\nCovert, Ian C. 2020. “Explaining by Removing: A Uniﬁed Framework for Model Explanation.” arXiv:2011.14878 [Cs], 90.\n\n\nJanzing, Dominik, Lenon Minorics, and Patrick Blöbaum. 2019. “Feature Relevance Quantification in Explainable AI: A Causal Problem.” arXiv:1910.13413 [Cs, Stat], November. http://arxiv.org/abs/1910.13413.\n\n\nMerrick, Luke, and Ankur Taly. 2020. “The Explanation Game: Explaining Machine Learning Models Using Shapley Values.” arXiv:1909.08128 [Cs, Stat], June. http://arxiv.org/abs/1909.08128.\n\n\nShapley, L. 1953. “A Value for n-Person Games.” In Contributions to the Theory of Games, 2:307–17. Princeton, NJ: Princeton University Press.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "framework.html",
    "href": "framework.html",
    "title": "2  Evaluation Framework",
    "section": "",
    "text": "In this section, we propose a human-centric framework for evaluating model explanation methods that is grounded in causal reasoning and the premise that explanations should align with an explainee’s objectives. An evaluation framework is necessary because the multiplicity of available methods makes it impossible to generate every possible explanation, effectively forcing practitioners (explainers) to select a subset of methods. However, to perform this selection in a principled way requires the explainer to define criteria for comparing methods. Since there are innumerable valid criteria, it is helpful to establish a principle, from which, criteria are derived. One approach, leveraged by Miller (2018), is to start from the premise that explanations should mirror human explanations. Consequently, model explanations that more closely resemble human explanations are considered better, which implicitly treats human explanations as a gold standard. In our view, this is a valid, but arguably objectionable standard. Instead, we propose the principle that every explanation should be aligned with the purpose of the explanation as defined by the explainee. The implication is that explanations that are more closely aligned with the explainee’s objectives are better, or more correct, than explanations that are misaligned. In this way, we avoid the pitfalls of prioritizing human-like explanations, while maintaining a human-centric approach.\nTo aid practitioners in generating human-centric explanations, we propose a four step process closely modeled after the formulate-approximate-explain (FAE) framework Merrick and Taly (2020). First, the explainer and explainee must specify a set of target explanatory questions that explanations should answer. Next, the explainer identifies an explanation-generating method aligned with each question. Third, the explainer generates the explanations and provides them to the explainee. In the final stage, the explanations are interpreted, evaluated, and the cycle may be repeated.\nEvery explanation is an answer to a question, so the first step in generating correct model explanations is to specify the question that the explanation should address. In our view, these questions can be elicited by the explainee, but should be governed by the explainee’s objectives. The explainer and explainee’s objectives may not always be aligned Mittelstadt, Russell, and Wachter (2019), necessitating a choice over whose objectives to prioritize. In our framework, we prioritize the explainee’s objectives. Since there are many potential explainees, each of which may have different objectives, these questions must be formulated on a per-case basis. A second consideration that the explainer must keep in mind is whether the specified questions are answerable under the relevant constraints of the explanation-generating process. These constraints could involve the explainer’s knowledge of the available methods, the time available for generating explanations, the explainee’s level of domain expertise, the acceptability of the assumptions required to generate the explanation, and many other factors.\nEvery target explanatory question has an associated level and type. By level, we refer to the idea introduced earlier that explanations can either consider the model independent of the real world, or attempt to account for real-world relationships. We will address exactly what this means in subsequent sections. A target explanatory question can also be one of three types: associative, interventional, and counterfactual. Together, these groups are referred to as the “ladder of causality” Pearl (2009). Although explanation and causality may seem like separate topics, they are highly intertwined Miller (2018). To make things concrete, consider a model used to predict risk of default as part of a loan application. The following are all possible explanatory questions:\n\nHow did race influence the model’s decision to approve the application?\nWhat if the user increases their income to \\(X'\\) from \\(X\\)?\nWould my loan application have been approved had my income been \\(X'\\) rather than \\(X\\)?\n\nEach of these questions implies a different explainee with different objectives: a model auditor interested in assessing the model for potential bias, an employee of the bank trying to understand the model’s behavior, and an individual interested in what might have happened under different circumstances.\nSpecifying a target question may require multiple iterations, in particular, to resolve any lingering ambiguity. For example, the intended level of explanation is not immediately clear with the current wording . We suggest that it is the explainer’s responsibility to resolve such ambiguity when possible. In situations where the explainer cannot interact directly with the explainee, the explainer may be forced to make assumptions about the explainees objectives.\nOnce the target questions have been specified, the next step is to select an explanation-generating method that addresses each question. In order to make this selection, the explainer must have a clear understanding of the specific types of questions that each method is capable of addressing. In reality, this stage occurs in parallel with the first, as the explainer must keep the association between explanatory methods and the questions they address in mind in order to assess the feasibility of answering the explainee’s target questions. We provide additional details and specific recommendations about which methods are most appropriate in different contexts in our discussion section.\nAfter a method has been identified, the explainer generates the explanation. The explainer must have a sufficiently deep understanding of the method to accurately assess whether the resulting explanation, when interpreted correctly, addresses the target question. There are a variety of ways that the two may become misaligned. For example, many explanation-generating methods rely on sampling rather than computing exact values such that if the estimator is biased, then the resulting explanations may not address the target question unless certain other assumptions are met. As with the selection step, these considerations should be kept in mind when specifying target questions during the first step. Functionally, this means that the explainer may need to communicate and assess the validity of any additional assumptions that are required at this step while specifying the target questions.\nThe final step is for the explainer to provide the explanations to the explainee, and – when possible – engage in a dialog with the explainee about the explanations. This interactive approach to providing explanations is aligned with Miller (2018), who suggests that conversation between explainee and explainer is necessary because XAI is fundamentally a human-agent interaction problem.\nGenerating correct explanations requires the explainer to have a deep understanding of the available methods. In particular, the explainer must have a mental catalog of the relevant methods, the types of explanatory questions that each addresses, any required assumptions, and other relevant considerations. Causality is central to this effort because the types of questions that typically motivate the desire for model explanations span the rungs of the ladder of causality.\n\n\n\n\nMerrick, Luke, and Ankur Taly. 2020. “The Explanation Game: Explaining Machine Learning Models Using Shapley Values.” arXiv:1909.08128 [Cs, Stat], June. http://arxiv.org/abs/1909.08128.\n\n\nMiller, Tim. 2018. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” arXiv:1706.07269 [Cs], August. http://arxiv.org/abs/1706.07269.\n\n\nMittelstadt, Brent, Chris Russell, and Sandra Wachter. 2019. “Explaining Explanations in AI.” Proceedings of the Conference on Fairness, Accountability, and Transparency, January, 279–88. https://doi.org/10.1145/3287560.3287574.\n\n\nPearl, Judea. 2009. Causality: Models, Reasoning, and Inference. Second. Cambridge University Press.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluation Framework</span>"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "3  Background",
    "section": "",
    "text": "3.1 Causality\nAlthough explanation and causality may at first appear to be separate topics, they are highly intertwined. Miller (2018) reviews theories of explanation from the social sciences and demonstrates that notions of causality are central. While there are different philosophical theories of causality and frameworks for estimating causal effects, the XAI literature primarily leverages the formal model of causality introduced by Halpern and Pearl (see Halpern and Pearl (2005a) and Halpern and Pearl (2005b)).\nUnder this framework, questions are divided into three levels – referred to as the ladder of causality – each of which requires a different degree and type of causal information.\nAt the lowest level are “how” questions, which are associative and can be answered purely from observational data without any causal information. The second rung includes “what if’’ questions, which involve reasoning about the effects of interventions. These types of questions require either a randomized trial or assumptions about which variables are causally related. On the highest wrung are “would have” questions that involve reasoning about counterfactual scenarios, that is, what would have occurred had a different action been taken under identical circumstances. These types of questions require knowledge of the functional (i.e. mathematical) equations governing each relationship. Intuitively, counterfactual questions are distinct from interventional ones because knowing the outcome of the action taken changes our beliefs about the likely outcome of other actions under the same circumstances. A more rigorous exploration of these different levels and their differences can be found in Pearl (2009). However, we note that for model-level explanations, the distinction between interventional and counterfactual questions is functionally irrelevant as the model itself is sufficient to answer both types of questions.\nIn order to apply a causal perspective to Shapley-based model explanations, some familiarity with the formalisms from the causality literature is required. The remainder of this section introduces those formalisms in a limited way. For a more complete introduction to causal inference, see Pearl, Glymour, and Jewell (2016). An exhaustive treatment of the topic can be found in Pearl (2009).\nThe primary tool employed to answer “how” questions are conditional probabilities, which require no causal assumptions or additional information. For many practitioners, the lack of additional assumptions and an ability to forgo the complications involved in causal reasoning may be ideal. However, questions at this level are the least likely to align with an explainee’s target questions since human explanations typically involve “why” questions (Miller (2018)).\nIn order to address “what if” questions on the second rung of the ladder of causality, practitioners must provide information about the causal relationships between variables. However, since the true causal relationships governing natural processes is unknown, this information is better framed as a set of causal assumptions. These assumptions are encoded using a formalism called a graphical causal model (GCM), which consists of a set of nodes and edges (\\(\\mathcal{G} = (V, E)\\)) that form a directed acyclic graph (DAG). The nodes of a GCM represent variables and each edge represents a causal relationship between the two variables. Just as importantly, a missing edge between two variables indicates the practitioner’s belief that no causal relationship exists.\nA graphical causal model allows a practitioner – under certain conditions – to predict the effects of interventions from observational data. Observational data alone is insufficient because correlation between variables has two potential sources: either there is a causal relationship between the variables or there is an additional variable (called a confounder) that induces a spurious (i.e. non-causal) correlation (see Figure 3.2). To understand how to differentiate between the two, it is helpful to first understand how a GCM can be used to predict the dependencies between variables.\nA graphical causal model is sufficient to determine how two variables are related in observational data corresponding to that GCM. Specifically, a GCM can indicate whether two variables are independent, independent conditional on other variables, dependent, or dependent conditional on other variables1. The simplest case are direct relationships, which indicate that two variables are unconditionally dependent. To assess implicit dependencies more generally, a GCM can be decomposed into a combination of three fundamental building blocks: chains, forks, and colliders (see Figure 3.1). Each of these building blocks implies a different set of dependencies, which can be expressed in terms of expectations over the random variables involved.\nIn a chain (Figure 3.1 (a)), four relationships can be deduced. There are three direct connections. It is also true that \\(Y\\) and \\(X_1\\) are independent, conditional on \\(X_2\\).\nIn a fork Figure 3.1 (b), there is a variable \\(X_1\\) that is a common cause of the other two. In addition to the the relationships implied by the direct edges, we also have the following relationships:\nIn a collider Figure 3.1 (c), there is a variable \\(Y\\) that is directly influenced by both of the other variables. Conditioning on a collider induces dependence between two otherwise-independent variables:\nTogether, these building blocks can be used to construct a complete set of dependencies implied by a GCM. The next step is to be able to differentiate between causal and non-causal sources of dependence.\nSpurious correlations can be identified by looking for “backdoor” paths between variables. In Figure 3.2, there is one direct relationship between \\(X_1\\) and \\(Y\\) and one backdoor path \\((X_1 \\leftarrow X_2 \\rightarrow Y)\\). Applying the rules above demonstrates how this path introduces additional non-causal dependence (correlation). A backdoor path can be closed in two ways: conditioning and colliders. The path \\(X_1 \\rightarrow X_2 \\rightarrow Y\\) is a chain and therefore (using the rules from earlier) conditioning on \\(X_2\\) makes \\(X_1\\) and \\(Y\\) independent, effectively “closing” that backdoor path. If there is a collider along a backdoor path, then that path is closed and conditioning on the collider has the negative consequence of opening the backdoor path. When there exists a set of variables \\(\\mathbf{Z}\\) that close all backdoor paths between a pair of variables (e.g. \\((X_1, Y)\\)), the backdoor criterion (Pearl (2009)) is satisfied and the interventional effect of \\(X_1\\) on \\(Y\\) can be estimated. However, the set of variables \\(\\mathbf{Z}\\) may not exist, meaning that the desired interventional quantity is not identifiable from observational data alone.\nG\n\n\n\nX1\n\nX1\n\n\n\nY\n\nY\n\n\n\nX1-&gt;Y\n\n\n\n\n\nX2\n\nX2\n\n\n\nX2-&gt;X1\n\n\n\n\n\nX2-&gt;Y\n\n\n\n\n\n\n\n\nFigure 3.2: Confounding in a Graphical Causal Model\nIntervening on a variable is equivalent to forcing it to take on some value irrespective of the other factors that would typically influence it. Graphically, this is equivalent to removing all of the incoming edges to the variable. For example, an intervention on \\(X_1\\) in Figure 3.3 (a) can be represented graphically by removing the edge \\((X_1, X_2)\\). Importantly, the graphs in Figure 3.3 (a) and Figure 3.3 (b) imply different sets of dependencies, which demonstrates a more general point that an interventional conditional distribution is not always equivalent to the corresponding observational conditional distribution. Notationally, the do-operator (Pearl (2009)) is used to differentiate between expectations over these two types of distributions: \\(E[Y=y | do(X_1=x_1)] \\neq\nE[Y=y | X_1=x_1]\\). To estimate the interventional effect of \\(X_1\\) on \\(Y\\), we condition on \\(X_2\\), which closes the only backdoor path \\(X_1 \\leftarrow X_2\n\\rightarrow Y\\), and therefore satisfies the backdoor criterion.\n\\[\\begin{align}\n    P(Y = y | do(X_1=x_1) &= \\sum_z P(Y=y | X_1=x_1, X_2 = x_2)P(X_2=x_2) \\\\\n    E[y| do(x_1)] &= \\sum_{x_2} E[y|x_1, x_2]p(x_2)\n\\end{align}\\]\nProvided that a set of variables satisfying the backdoor criterion can be identified, a GCM allows a practitioner to predict interventional effects from purely observational data thereby addressing “what if” questions on the second rung of the ladder of causality. However, a GCM alone is still insufficient to answer “why” questions, which require counterfactual reasoning.\nIn order to evaluate counterfactual scenarios associated with questions at the top of the ladder of causality, a structural causal model (SCM) is often required. Whereas a GCM only requires information about whether or not causal relationships between pairs of variables exist, a SCM requires the functional (e.g. mathematical) equations governing those relationships. Formally, a SCM It is comprised of four components: a set of exogenous “noise” variables \\(U\\) whose values are assumed to be determined outside of the SCM, a joint distribution \\(P_U\\) over these exogenous variables, a set of endogenous variables \\(V\\) whose values are determined by the SCM, and a set of functions \\(F = \\{f_1, f_2,\n...\\}\\) where each \\(f_i\\) assigns values to variables in \\(V\\) based on the values of the other variables. Every SCM has an associated GCM whose nodes are the endogenous variables and the directed edges represent the causal relationships captured by the equations in \\(F\\). For simplicity, exogenous variables are typically omitted from the GCM. Every endogenous variable must be a descendent of at least one exogenous variable and exogenous variables cannot be descendants of any other variable. Therefore, each endogenous variable can be written as a function of its parents (denoted \\(Pa(\\cdot)\\)) and the associated noise variable:\n\\[\nX_i = f_i(Pa(X_i), U_i)\n\\]\nWith this foundation, we can now see how model level questions at any level can be addressed using only the model (Figure 3.4). For simplicity, assume we have just two features \\(X_1, X_2\\) with associated noise terms \\(U_1,\nU_2\\). There are three endogenous variables \\(V = \\{Y, X_1, X_2\\}\\) representing the 2 features and the output of the model. Since we are interested in a model level explanation, every feature \\(X_i\\) has an edge pointing to \\(\\hat{Y}\\) and no other edges between them. Therefore, \\(f_1, f_2\\) are functions of the noise terms only and \\(f_{Y}\\) is the model itself, which is deterministic and therefore has no additional noise term. The SCM is fully specified, so counterfactual questions can be addressed, which means that interventional and associative questions are also addressable.\nFollowing a similar argument, we can also see how world-level explanations cannot always be addressed without additional information. Given the same setup as in the previous example, but with the edge \\((X_2, X_1)\\) (see Figure 3.4 (b)). We now have \\(Pa(X_1) = \\{X_2, U_1\\}\\), however, we have not specified a corresponding function \\(f_1\\) that includes both of these terms. Therefore, the SCM is under-specified and counterfactual questions cannot be addressed. However, interventional questions can be addressed because there exists a set of variables satisfying the backdoor criterion for estimating the two interventional quantities of interest: \\(P(Y = y | do(X_1 = x_1))\\) and \\(P(Y\n= y | do(X_2 = x_2)\\). For the first case, we condition on \\(X_2\\) to close the backdoor path \\((X_1, X_2, Y)\\). In the second case, no conditioning is required since the post-intervention graph is the same as the pre-intervention graph.\nIn this section, we reviewed the core components of Pearl-style causality, namely, the degree and type of causal information required to address questions on different rungs of the ladder of causality. We have also provided an initial preview of how these formalisms (GCMs and SCMs) are connected to the task of generating model explanations. For explanatory questions targeting world-level model explanations, the usual hierarchy of required causal information applies. However, for model-level explanations, the model itself is sufficient for generating explanations that can address questions at all rungs. Therefore, from a causal perspective, understanding the desired level of explanation is essential to understand whether any auxiliary causal information is required. We will return to these ideas in later sections, but first, some additional background on Shapley values is required.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#causality",
    "href": "background.html#causality",
    "title": "3  Background",
    "section": "",
    "text": "G\n\n\n\nX1\n\nX1\n\n\n\nX2\n\nX2\n\n\n\nX1-&gt;X2\n\n\n\n\n\nY\n\nY\n\n\n\nX2-&gt;Y\n\n\n\n\n\n\n\n\n(a) Chain\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nX1\n\nX1\n\n\n\nX2\n\nX2\n\n\n\nX1-&gt;X2\n\n\n\n\n\nY\n\nY\n\n\n\nX2-&gt;Y\n\n\n\n\n\n\n\n\n(b) Fork\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nX1\n\nX1\n\n\n\nY\n\nY\n\n\n\nX1-&gt;Y\n\n\n\n\n\nX2\n\nX2\n\n\n\nX2-&gt;Y\n\n\n\n\n\n\n\n\n(c) Collider\n\n\n\n\n\n\n\n\nFigure 3.1: Building Blocks of a Graphical Causal Model\n\n\n\n\n\\((X_2, X_1)\\) are dependent: \\(E[X_2 | X_1] \\neq E[X_2]\\)\n\\((Y, X_2)\\) are dependent: \\(E[Y|X_2] \\neq E[Y]\\)\n\\((Y, X_1)\\) are dependent: \\(E[Y|X_1] \\neq E[Y]\\)\n\\((Y, X_1)\\) are independent, conditional on \\(X_2\\): \\(E[Y|X_1, X_2] = E[Y|X_2]\\)\n\n\n\n\\((Y, X_2)\\) are dependent: \\(E[Y|X_2] \\neq E[Y]\\)\n\\((Y, X_2\\) are independent, conditional on \\(X_1\\): \\(E[Y|X_1, X_2] = E[Y|X_1]\\)\n\n\n\n\\(X_1\\) is independent of \\(X_2\\): \\(E[X_1|X_2] = E[X_1]\\)\n\\(X_1\\) and \\(X_2\\) are dependent, conditional on \\(Y\\): \\(E[X_1|X_2, Y] \\neq\nE[X_1|Y]\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nX1\n\nX1\n\n\n\nY\n\nY\n\n\n\nX1-&gt;Y\n\n\n\n\n\nX2\n\nX2\n\n\n\nX2-&gt;X1\n\n\n\n\n\nX2-&gt;Y\n\n\n\n\n\n\n\n\n(a) Pre-Intervention\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nX1\n\nX1\n\n\n\nY\n\nY\n\n\n\nX1-&gt;Y\n\n\n\n\n\nX2\n\nX2\n\n\n\nX2-&gt;Y\n\n\n\n\n\n\n\n\n(b) Post-Intervention\n\n\n\n\n\n\n\n\nFigure 3.3: Interventions Modify the Graphical Causal Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nX1\n\nX1\n\n\n\nY\n\nY\n\n\n\nX1-&gt;Y\n\n\n\n\n\nX2\n\nX2\n\n\n\nX2-&gt;Y\n\n\n\n\n\n\n\n\n(a) Model Level\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nX1\n\nX1\n\n\n\nY\n\nY\n\n\n\nX1-&gt;Y\n\n\n\n\n\nX2\n\nX2\n\n\n\nX2-&gt;X1\n\n\n\n\n\nX2-&gt;Y\n\n\n\n\n\n\n\n\n(b) World Level\n\n\n\n\n\n\n\n\nFigure 3.4: Levels of Explanation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#shapley-value",
    "href": "background.html#shapley-value",
    "title": "3  Background",
    "section": "3.2 Shapley Value",
    "text": "3.2 Shapley Value\nGame theory explores the strategic behavior and interactions between rational agents in the context of well-defined games. Cooperative game theory is a sub-field that focuses on games in which groups of players (coalitions) compete against other groups of players and receive a combined payout (gain), or alternatively, incur a total cost2. A cooperative game is defined by a set of players called the grand coalition \\(\\mathcal{C} = \\{1, 2,\n..., N\\}\\) and a value function3 \\(v\\), which specifies the real-valued payout every group of players \\(S \\subseteq \\mathcal{C}\\) receives when they cooperate. More formally, \\(v\\) is a set function \\(v: \\mathcal{P}(C) \\rightarrow \\mathbb{R}\\) such that \\(v(S) \\in \\mathcal{R}\\) and where \\(\\mathcal{P}(\\mathcal{C})\\) denotes the power set of the grand coalition. Since the payout is received by the group, there is a further complication regarding how to fairly allocate the group’s winnings to individual players, referred to as the attribution problem.\nThe Shapley value (Shapley (1953)) is a solution concept from cooperative game theory that produces an allocation strategy for fairly distributing payoffs to each player in a coalition. It can be viewed as the marginal contribution of each player to the coalition, averaged over all possible orderings of the players. To build intuition for how this solves the attribution problem, consider the following example.\nA factory employs three workers \\((w_1, w_2, w_3)\\), each of whom has agreed to be paid in proportional to their contribution to the factory’s widget output. Employees are required to work a fixed number of hours each week, but they are allowed to set their own schedules. The factory owner has decided to use Shapley values to set each employee’s wage and has collected data on factory output when different workers are present. In this scenario, the workers are the players in the game and the value function is captured by the hourly factory production for every combination of workers (see Table 3.1).\n\n\n\nTable 3.1: Worker Productivity\n\n\n\n\n\nworkers\nwidgets/hour\n\n\n\n\n\\(\\emptyset\\)\n0\n\n\n\\(w_1\\)\n5\n\n\n\\(w_2\\)\n5\n\n\n\\(w_3\\)\n5\n\n\n\\(w_1, w_2\\)\n8\n\n\n\\(w_1, w_3\\)\n10\n\n\n\\(w_2, w_3\\)\n6\n\n\n\\(w_1, w_2, w_3\\)\n12\n\n\n\n\n\n\nThe Shapley value for each worker is the number of additional widgets per hour (marginal contribution) produced when they arrive at the factory, averaged over all possible arrival orders. The marginal contribution of each worker \\(i\\) to a coalition \\(S\\) (columns 2-4 of Table 3.1) is the additional value generated when worker \\(i\\) arrives4.\n\\[\n\\Delta(i, S) = v(S \\cup i) - v(S)\n\\tag{3.1}\\]\nLet \\(\\Pi\\) denote the set of all orderings of the players in \\(C\\). Given player \\(i\\) and a permutation \\(\\pi \\in \\Pi\\), let \\(S\\) be the set of all players preceding player \\(i\\) in \\(\\pi\\):\n\\[\nS_{i, \\pi} = \\{j: \\pi(j) &lt; \\pi(i)\\}\n\\] {# eq-arrival-order}\nThe Shapley value for player \\(i\\) can then be expressed as:\n\\[\n\\phi(i) = \\frac{1}{N!} \\sum_{\\pi \\in \\Pi} \\Delta(i, S_{i, \\pi})\n\\tag{3.2}\\]\nThe last row of Table 3.2 computes the Shapley value for each worker (denoted as \\(\\phi_i\\)) using this formulation, which relies on the information in Table 3.1.\n\n\n\nTable 3.2: Shapley Value by Worker\n\n\n\n\n\n\n\n\n\n\n\norder\n\\(\\Delta(w_1)\\)\n\\(\\Delta(w_2)\\)\n\\(\\Delta(w_3)\\)\n\n\n\n\n\\(w_1, w_2, w_3\\)\n5\n3\n4\n\n\n\\(w_1, w_3, w_2\\)\n5\n2\n5\n\n\n\\(w_2, w_1, w_3\\)\n3\n5\n4\n\n\n\\(w_2, w_3, w_1\\)\n6\n5\n1\n\n\n\\(w_3, w_1, w_2\\)\n5\n2\n5\n\n\n\\(w_3, w_2, w_1\\)\n6\n1\n5\n\n\nShapley Value\n\\(\\phi_1 = \\frac{5+5+3+6+5+6}{6} = 5\\)\n\\(\\phi_2 = \\frac{3+2+5+5+2+1}{6} = 3\\)\n\\(\\phi_3 = \\frac{4+5+2+1+5+5}{6} = 4\\)\n\n\n\n\n\n\nThe Shapley value can be expressed in an equivalent way based on the number of unique subsets of the grand coalition \\(S \\subseteq C\\) and the number of permutations of \\(C\\) for which some ordering of players in \\(S\\) immediately precedes the \\(i\\)th player. The initial proof of the equivalence of these two formulations can be found in Shapley (1953) and is reproduced in an expanded fashion in Štrumbelj, Kononenko, and Robnik Šikonja (2009).\n\\[\n\\phi(i) = \\frac{1}{N!} \\sum_{S \\subseteq C \\backslash \\{i\\}} |S|!(N-|S| - 1)! \\Delta(i, S)\n\\tag{3.3}\\]\nOne of the primary motivations for the use of the Shapley value is that it can be derived axiomatically. Given the following three axioms, the Shapley value not only solves the attribution problem, but can be shown to be the unique solution.\n\nEfficiency: The Shapley values for individual players sum up to the payout received by the grand coalition: \\(\\sum_{i \\in C} \\phi(i) = v(C) -\nv(\\emptyset)\\).\nSymmetry: If two players make equal contributions to all possible coalitions, then they should receive equal payouts. For players \\(i\\) and \\(j\\), if \\(\\Delta(i, S) = \\Delta(j, S)\\) for all subsets \\(S \\subset C\\), then \\(\\phi(i) = \\phi(j)\\).\nDummy/Nullity/Sensitivity: If a player’s marginal contribution to all coalitions is zero (e.g. they never increase the payout of any coalition), then they should receive zero payout. For any player \\(i\\), if \\(\\Delta(i, S) =\n0\\) for all \\(S \\subset N\\), then \\(\\phi(i) = 0\\).\nLinearity/Additivity: Given two games defined by value functions \\(v\\) and \\(v'\\), the Shapley value for each player in the combined game is the sum of the allocations in each individual game: \\(\\phi_{v' + v}(i) = \\phi_v(i) +\n\\phi_{v'}(i)\\).\n\nWhile this axiomatic grounding is both powerful and compelling, it is not factored into our human-centric evaluation framework and therefore plays a rather limited role in our discussion of Shapley-based model explanations. With the necessary background on Shapley values and Pearl-style causality in place, we are now in a position to examine the Shapley explanation literature using this proposed framework.\n\n\n\n\nHalpern, Joseph Y., and Judea Pearl. 2005a. “Causes and Explanations: A Structural-Model Approach. Part I: Causes.” The British Journal for the Philosophy of Science 56 (4): 843–87. http://www.jstor.org/stable/3541870.\n\n\n———. 2005b. “Causes and Explanations: A Structural-Model Approach. Part II: Explanations.” The British Journal for the Philosophy of Science 56 (4): 889–911. https://www.jstor.org/stable/3541871.\n\n\nMiller, Tim. 2018. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” arXiv:1706.07269 [Cs], August. http://arxiv.org/abs/1706.07269.\n\n\nPearl, Judea. 2009. Causality: Models, Reasoning, and Inference. Second. Cambridge University Press.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. Wiley.\n\n\nShapley, L. 1953. “A Value for n-Person Games.” In Contributions to the Theory of Games, 2:307–17. Princeton, NJ: Princeton University Press.\n\n\nŠtrumbelj, E., I. Kononenko, and M. Robnik Šikonja. 2009. “Explaining Instance Classifications with Interactions of Subsets of Feature Values.” Data & Knowledge Engineering 68 (10): 886–904. https://doi.org/10.1016/j.datak.2009.01.004.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#footnotes",
    "href": "background.html#footnotes",
    "title": "3  Background",
    "section": "",
    "text": "There are certain cases where these dependencies do not hold, so it is more accurate to say that the variables are likely dependent. For the ease of exposition, we exclude this modifier.↩︎\nOther work use the terms coalitional game theory, and correspondingly, coalitional games↩︎\nAlso referred to as a characteristic or contribution function↩︎\nThere are different ways to indicate the relationship between \\(i\\), \\(S\\), and \\(v\\). For example, \\(\\Delta_v(i, S)\\), \\(\\Delta_{i, S}(v)\\), and $ _{i, S}$ are all used in the literature. However, since a cooperative game is partially defined by the value function, we choose to leave the dependence on \\(v\\) implicit in order to simplify the notation↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "explanations.html",
    "href": "explanations.html",
    "title": "4  Shapley Explanations",
    "section": "",
    "text": "4.1 Overview\nThe key differentiator between Shapley explanations is how the underlying game – as defined by the payout, players, and the value function – is formulated. Before getting into the specifics of each method, we review how each of these components has been adapted to the task of generating model explanations, and in the case of specifying a value function, how this choice can be reframed using a causal perspective in the context of our evaluation framework.\nThe definition of the payout is directly related to the scope of the resulting explanation. In cooperative game theory, the payout is the value received by the coalition of players when they cooperate. When applied to model explanations, the payout is a numeric attribute of the model such as its accuracy, variance explained, or the value that the model predicts for a particular input. When the payout is defined in terms of a model prediction, the result is a local explanation, which is an explanation of a particular instance. In contrast, global explanations address model behavior in the aggregate. Payouts defined in terms of model performance (e.g. accuracy, variance explained, etc.) lead to global explanations. However, in some cases, global explanations can be constructed from local ones, for example, by averaging local explanations across all predictions. In this work, we are primarily interested in Shapley methods that yield local explanations; however, we briefly cover methods that generate global explanations as well.\nIn the vast majority of cases, a model’s features are treated as the players in the game and groups of players constitute a coalition. When reviewing a particular method, we will omit this detail from our discussion except in those cases where a different formulation is used.\nFor the remainder of this paper we use the following notation1:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Shapley Explanations</span>"
    ]
  },
  {
    "objectID": "explanations.html#overview",
    "href": "explanations.html#overview",
    "title": "4  Shapley Explanations",
    "section": "",
    "text": "\\(D\\): The set of features \\(\\{1, 2, ..., D\\}\\)\n\\(S\\): A subset of features, \\(S \\subseteq D\\)\n\\(f\\): The machine learning model, \\(f(x_1, x_2, ..., x_N)\\)\n\\(\\mathbf{X}\\): a multivariate random variable \\(\\{X_1, X_2, ..., X_N\\}\\)\n\\(\\mathbf{x}\\): a set of values \\(\\{x_1, x_2, ..., x_N\\}\\)\n\\(\\mathbf{X}_S\\): the set of random variables \\(\\{X_i: i \\in S\\}\\)\n\\(\\mathbf{X}_{\\bar{S}}\\): the set of random variables \\(\\{X_i: i \\notin S\\}\\)\n\n\\(\\mathbf{x}_S\\): the set of values \\(\\{x_i: i \\in S\\}\\)\n\\(\\mathbf{x}_{\\bar{S}}\\): the set of values \\(\\{x_i: i \\notin S\\}\\)\n\n\n4.1.1 Value Function\nThe most consequential aspect of the game formulation is the value function. As noted previously, the value function specifies the payout for every subset of players. Since most machine learning models cannot make predictions when inputs are missing (as is the case for any coalition besides the grand coalition), the value function must provide a replacement value for every feature that is not part of the coalition. In essence, it must simulate what happens when features are removed from the model (Janzing, Minorics, and Blöbaum (2019), Merrick and Taly (2020), Covert (2020)). The brute-force approach is to avoid simulating altogether and apply the same learning algorithm (and hyperparameters) to every subset of features, effectively training \\(2^N\\) separate models where \\(N\\) is the number of features. Other alternatives include replacing the missing value with a fixed value (e.g. zero), a value from a reference data point, or the expected value over a collection of data points (reference distribution). Each of these alternatives to the brute force approach can be thought of as a special case of the last option (Merrick and Taly (2020), Sundararajan and Najmi (2020)). However, there are numerous ways to select a reference distribution, which has generated significant debate over which choice is the correct one.\nWithin our evaluation framework, the correct reference distribution is the one aligned with the target explanatory question, which is defined by the intended level of explanation and whether it is associative, interventional, or counterfactual. Shapley-based methods can be used to provide either model or world level explanations. However, much of the Shapley explanation literature implicitly assumes one of the two and fails to recognize the existence or validity of the other. There are two notable exceptions: Basu (2020) makes a similar distinction using what they call “modes of interpretation” and Wang, Wiens, and Lundberg (2021) introduce the notion of “boundaries of explanation,” which captures a similar idea. Failure to clearly distinguish between these different levels of explanation has fueled the debate over which reference distribution is correct. Before discussing particular methods, we revisit the different classes of reference distributions used by various Shapley methods and the types of explanatory questions that each can address.\nReference distributions can be categorized as unconditional or conditional based on whether they consider features jointly or independently. In cases where features are independent – a situation that is rarely, if ever, true in practice – this distinction is irrelevant and the resulting Shapley values for both types are equivalent. Conditional value functions can be further categorized based on whether they are observational or interventional. Each of these three classes of reference distribution (unconditional, observational conditional, and interventional conditional) yield Shapley explanations at a particular level (model or world) that can be used to address particular types of target explanatory questions when interpreted correctly (see Figure 4.1).\n\n\n\n\n\n\nflowchart\n  A{Level}\n  A --&gt;|model| B[Marginal]\n  A --&gt;|World| C{question}\n  C --&gt;|associative| D[observational]\n  C --&gt;|interventional| E[interventional]\n  C --&gt;|counterfactual| F[interventional]\n\n\n\n\nFigure 4.1: Selecting A Reference Distribution\n\n\n\n\n\nValue functions based on an observational conditional distribution replace missing features with values that are consistent with the observed relationships among features.\n\\[\nv(S) = E[f(\\mathbf{X}) | \\mathbf{X}_S = \\mathbf{x}_s]\n\\tag{4.1}\\]\nThis choice of reference distribution is unable to distinguish between correlation that is due to a causal effect and correlation due to confounding (i.e. spurious correlation). As a result, the Shapley values generated by methods that use this formulation are only able to address associative world-level target explanatory questions.\nIn contrast, value functions based on an interventional conditional distribution replace missing features with values that are consistent with the causal relationships between features.\n\\[\nv(S) = E[f(\\mathbf{X}) | do(\\mathbf{X}_S = \\mathbf{x}_s)]    \n\\tag{4.2}\\]\nHowever, methods based on this type of value function may require the practitioner to provide auxiliary causal information depending on the type of explanatory question. As noted in the background section on causality, interventional questions require a GCM and counterfactual questions require an SCM.\nFinally, value functions based on an unconditional reference distribution replace individual feature values independent of the values of other features.\n\\[\nv(S) = E[f(\\mathbf{x}_S, \\mathbf{X}_{\\bar{S}})]\n\\tag{4.3}\\]\nIn our view, this type of reference distribution is appropriate for generating model-level explanations as it effectively ignores real-world causal relationships by assuming feature independence. As noted previously, the model itself is sufficient for addressing associative, interventional, and counterfactual questions so no auxiliary causal information is required.\nJanzing, Minorics, and Blöbaum (2019) make the case that a causal perspective supports using a marginal reference distribution in all cases. To make their point, they introduce a distinction between real-world (“true”) features \\(\\tilde{X_1},\n\\tilde{X_2}, ..., \\tilde{X_n}\\) and features that serve as input to the model \\(X_1, X_2, ..., X_n\\) (see Figure 4.2). Model features have no causal relationships (e.g. are independent) even if causal relationships exist between their real-world counterparts. Using this setup, they show that the backdoor criterion is satisfied and the interventional conditional value function (Equation 4.2}) yields identical Shapley values as an unconditional one (Equation 4.3}).\n\n\n\n\n\n\nFigure 4.2: Model vs. World Causality\n\n\n\nIn our view, the causal perspective assumed by Janzing, Minorics, and Blöbaum (2019) is limited because it implicitly assumes that all target explanatory questions can be addressed through model-level explanations. To illustrate the point, consider a model that takes loan amount, income, and savings account balance as inputs and predicts risk of default. The question “What if I resubmit by loan application and say my income is \\(X\\)?’’ likely has a model-level intention, while a question like “What if I increase my income to \\(X\\)” is targeting a world-level explanation. If higher income is causally related to savings account balance (e.g. always depositing a fixed percentage of income), the answer to the second question is not necessarily the same as the first. In our evaluation framework, part of what makes an explanation correct is that it aligns with the target explanatory question. Therefore, understanding the explainee’s desired level of explanation is critical\nThe adoption of the Janzing-style causal perspective with its failure to adequately distinguish between levels of explanation has led to an unfortunate pattern in the literature where the terms interventional, marginal, and unconditional are used interchangeably. Since this equivalence only applies to model-level explanations, we use the term interventional only when a causal perspective is intentionally adopted.\n\n\n4.1.2 Indirect Influence\nDifferentiating between model and world-level explanations also helps to resolve the ongoing “indirect influence” debate in the Shapley explanation literature. The key question behind this debate is whether a real-world feature that exhibits only an indirect influence on the model should be assigned a Shapley value of zero. Another way to frame this question is: should a feature that is not functionally used by the model be considered irrelevant when providing a model explanation?\nOne school of thought argues that Shapley-based attributions should assign zero importance to irrelevant features. Let \\(X_1, X_2\\) be two features and consider the model \\(f(x_1, x_2) = x_2\\). The model \\(f\\) clearly does not functionally depend on \\(x_1\\) and therefore, the argument goes, \\(x_1\\) is irrelevant. Numerous authors have demonstrated that when a value function based on a conditional expectation (observational or interventional) is used (see example 3.3 from Sundararajan and Najmi (2020), section 2 from Merrick and Taly (2020), and example 1 from Janzing, Minorics, and Blöbaum (2019)), irrelevant features may receive non-zero attributions. They argue that a non-zero attribution is both counterintuitive and constitutes an apparent violation of the dummy axiom. Merrick and Taly (2020) demonstrates the practical implications of this violation for assessing fairness. Consider two models that make decisions about who should be hired by a moving company. Model A exclusively considers the applicant’s gender, a protected category, while model B takes both gender and the applicant’s lifting capacity into account. If gender and lifting capacity are correlated and a conditional method is used, both gender and lifting capacity receive non-zero attributions in model A even though lifting capacity is functionally irrelevant. They argue that these attributions hide the degree of bias in the model’s decisions. Sundararajan and Najmi (2020) provides a different view, suggesting that these attributions may lead practitioners to incorrectly believe that a model is sensitive to a protected feature. It is also possible to construct scenarios in which attributions appear to violate the symmetry (Sundararajan and Najmi (2020) and Merrick and Taly (2020)), and linearity (Sundararajan and Najmi (2020)) axioms. Opponents of methods based on a conditional value function use these violations and their implications for assessing model fairness to support the use of marginal SHAP methods, which will never assign a non-zero attribution Merrick and Taly (2020) to an irrelevant feature.\nModel fairness considerations have also been used to justify conditional Shapley methods. Adler et al. (2016) propose a method for auditing black box models for indirect influence, which they argue has implications for assessing algorithmic fairness. They provide an example of auditing a model used to approve home loans to ensure that race does not have an undue influence on the outcome. Even if the model does not functionally depend on race, it may include other variables (e.g. zipcode) that serve as proxies for race, allowing race to have an indirect influence on the model. From this standpoint, conditional Shapley methods that assign non-zero attributions to features that are not explicitly included in the model is a desirable property.\nBoth sides acknowledge that the choice of value function is directly related to the indirect influence debate. The underlying implication in the literature is that practitioners should use their belief about the right solution to the indirect influence debate to select a value function. In our view, the value function should be selected to align with the target explanatory question, which requires understanding the level of explanation that is sought. If a model-level explanation is desired, then an unconditional value function is appropriate and, as a result, irrelevant features will receive zero attributions. Conversely, if it is a world-level explanation that that explainee is after, then a conditional method is appropriate and it is not only permissible, but desirable, that irrelevant features receive non-zero attributions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Shapley Explanations</span>"
    ]
  },
  {
    "objectID": "explanations.html#global-explanatation",
    "href": "explanations.html#global-explanatation",
    "title": "4  Shapley Explanations",
    "section": "4.2 Global Explanatation",
    "text": "4.2 Global Explanatation\nShapley-based model explanations arose out of the relative feature importance literature where the resulting importances can be viewed as a global model explanation. These methods formulate the game by treating features as players and the total variance explained as the payout. We briefly review these historical roots because the developments in this literature foreshadow those that occurred later when Shapley-based methods were developed to generate local explanations.\n\n4.2.1 Linear Models\nThe earliest Shapley-based model explanation methods were developed to quantify relative feature importance for linear models2. The first such method (LMG) was made known by Kruskal (1987), but originated with Lindeman, Merrenda, and Gold (1980), who suggested that relative feature importance be computed by averaging the contributions of each feature to the total variance explained over all possible orderings of the features. They justified this computationally-expensive approach by demonstrating that other approaches for computing relative feature importance (e.g. comparing the magnitude of the regression coefficients or decomposing total variance using semipartial correlations) yield different values depending on the order in which features are considered. However, the connection between LMG and the Shapley value was not made explicit until Stufken (1992). In what was largely a re-invention of LMG, Lipovetsky and Conklin (2001) introduced Shapley Net Effects, but explicitly appealed to the axiomatically-grounded Shapley value from cooperative game theory to justify their approach. One critique of LMG and Shapley Net Effects was that functionally-irrelevant features could receive a non-zero relative importance when features are dependent. In response, Feldman (2005) introduced the proportional marginal variance decomposition PMVD method, which weights permutations of features in a data-dependent way such that functionally-irrelevant features are assigned zero importance. These concerns are the precursor to the indirect influence debate, albeit without that particular terminology.\n\n\n4.2.2 Black-Box Models\nA related, but more general, line of research uses the Shapley value to estimate relative feature importance for arbitrary black-box models by attributing the model’s total variance explained to individual features. Owen (2014) introduced a Shapley-based method for decomposing the variance of the output of a model, which was later named Shapley Effect by Song, Nelson, and Staum (2016). To simplify computation, Shapley Effect assumes feature independence and computes Sobol indices in order to provide an upper and lower bound for the exact Shapley value. Recognizing the limitation of assuming feature independence, Song, Nelson, and Staum (2016) extended this approach to handle dependent features. They propose an algorithm to approximate the Shapley value that extends Castro, Gómez, and Tejada (2009) and involves two levels of sampling in order to estimate the necessary conditional variances: sampling feature permutations and sampling from the empirical distribution. Owen and Prieur (2017) provides conceptual backing to Song, Nelson, and Staum (2016), making the case that the Shapley value is the correct approach to estimating feature importance when features are dependent. The primary alternative, they argue, is a version of ANOVA, which avoids the feature independence assumption, but introduces conceptual problems. First, importances can be negative, which the authors argue is counter-intuitive. Moreover, the possibility of negative importances allows for a variable that is not functionally used by a model to receive non-zero importance. Owen and Prieur (2017) argues that the Shapley value is preferred because it avoids both of these limitations.\nQuestions about how to handle dependent features and whether functionally-irrelevant features should be attributed importance drove methodological advancements around the use of Shapley values to generate global model explanations. In both the linear and black-box settings, the earliest methods assumed feature independence with more complicated methods that could account for feature dependence coming later. Both literatures also contain arguments about the proper way to handle features that have an indirect influence on the output. In fact, one of the motivations for PMVD is a concern over the fact that LMG violates the dummy axiom (referred to as the exclusion axiom in the original work), which says that functionally irrelevant features should receive zero importance Grömping (2007). In discussing the merits of this concern, Grömping (2007) notes that the relevance of the dummy axiom depends on the purpose behind computing relative feature importance. If the purpose is to understand how much a feature contributes to a model’s predictions (e.g. a model-level explanation), then a feature that is not functionally used by the model should receive zero importance. On the other hand, if the purpose is to understand how real-world interventions impact the model (world-level explanations), then assigning non-zero importances to functionally irrelevant features is justified. As the previous section hopefully makes clear and the subsequent section will expand upon, each of these trends and debates have an analog in the Shapley-based local explanation literature.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Shapley Explanations</span>"
    ]
  },
  {
    "objectID": "explanations.html#local-explanations",
    "href": "explanations.html#local-explanations",
    "title": "4  Shapley Explanations",
    "section": "4.3 Local Explanations",
    "text": "4.3 Local Explanations\nIn this section, we review Shapley-based methods for generating local explanations through the lens of our evaluation framework. Much like the Shapley-based global explanation literature, the methods and associated debates in this literature are largely driven by two concerns: how to generate explanations when features are dependent, and whether features that the model does not explicitly depend upon should be part of an explanation. Our goal is to provide enough information about each of the methods – the type of explanatory questions they address, their underlying assumptions, and their limitations – to allow practitioners to produce correct Shapley-based explanations. For a summary of the methods, see Table 4.1.\n\n\n\nTable 4.1: Local Shapley Explanation Methods\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nDefinition\nEstimation\nLevel\nQuestion\n\n\n\n\nShapley Regression Values (SRV)\nobservational conditional\nobservational conditional\nworld\nassociative\n\n\nShapley Sampling Values (SSV)\nobservational conditional\nunconditional\nmodel\ncounterfactual\n\n\nKernelSHAP\nobservational conditional\nunconditional\nmodel\ncounterfactual\n\n\nConditional Kernel SHAP\nobservational conditional\nobservational conditional\nworld\nassociative\n\n\nBaseline Shapley (BSHAP)\nN/A\nN/A\nN/A\nN/A\n\n\nQuantitative Input Influence (causal-QII)\nunconditional\nunconditional\nmodel\ncounterfactual\n\n\nDistal Asymmetric Shapley Values (d-ASV)\nobservational conditional\n\nN/A\nN/A\n\n\nProximate Asymmetric Shapely Values (p-ASV)\nobservational conditional\n\nN/A\nN/A\n\n\nCausal Shapley Values (CSV)\ninterventional conditional\ninterventional conditional\nboth\ninterventional\n\n\nShapley Flow (SF)\nN/A\nN/A\nboth\ncounterfactual\n\n\nRecursive Shapley Values (RSV)\nN/A\nN/A\nboth\ncounterfactual\n\n\n\n\n\n\n\n4.3.1 Observational and Implicitly-Causal Methods\nThe earliest and most commonly-used methods leverage a purely observational approach to generating explanations. Many of these methods define a value function in one way, but estimate Shapley values that correspond to a different value function Kumar et al. (2020). For the purposes of evaluating the correctness of the resulting explanations, what matters is that the estimated Shapley values are aligned with the target explanatory questions. Most of these methods rely on some form of sampling to approximate the Shapley values. Following Merrick and Taly (2020), we advocate for quantifying the uncertainty in these estimates as part of generating the explanations. Merrick and Taly (2020) propose computing confidence intervals for the estimates; however, we note that other methods from the uncertainty quantification literature are worth exploring. Unfortunately, a review of those methods is outside the scope of this work.\n\n4.3.1.1 Shapley Regression Values\nE. Štrumbelj, Kononenko, and Robnik Šikonja (2009) proposed Interactions Methods for Explanations (IME), the earliest method for generating Shapley-based local explanations. The authors define the target Shapley values using a conditional observational value function (Equation 4.1) and estimate them by fitting separate models for each subset of features. Covert (2020) showed that this brute-force estimation procedure yields values that are aligned with the defined value function. Based on the estimation procedure, subsequent work refers to these as Shapley Regression Values (SRV). The resulting explanations are only able to address target explanatory questions on the first rung of the ladder of causality (associative/”how” questions).\n\n\n4.3.1.2 Shapley Sampling Values\nIn follow up work, Strumbelj and Kononenko (2010) proposed a method that simulates feature removal using a product of uniform distributions, where the bounds for each uniform distribution are determined based on the minimum and maximum values in the training data. The resulting Shapley values are referred to as Shapley Sampling Values (SSV).\nLet \\(\\mathcal{U(X_i)}\\) refer to the uniform distribution associated with feature \\(i\\):\n\\[\n\\hat{v}(S) = E_{\\Pi_{i\\in D} \\mathcal{U}(\\mathbf{X}_i)}[f(\\mathbf{x}_S, \\mathbf{X}_{\\bar{S}}]\n\\tag{4.4}\\]\nThe estimation procedure for SSV solves two problems with SRV: it does not require retraining an exponential number of models and does not require full access to the training data. However, since SSV relies on sampling to estimate the Shapley values, it is important – under our evaluation framework – to assess the variability of the resulting Shapley values. Merrick and Taly (2020) propose a method for generating confidence intervals for Shapley values that could be used. Methods from the uncertainty quantification literature are also relevant for this effort. The computational benefits of SSV also come at the cost of generating Shapley values that do not align with the intended value function (observational conditional) unless the features are independent. Therefore, the resulting explanations should not be used to answer associative world-level explanatory questions unless this assumption is validated. However, because the estimation procedure yields values that are unbiased with respect to an unconditional value function, they can be used (without additional assumptions) to address associative, interventional, and counterfactual model-level explanatory questions. The ability to do this comes from the fact that the model itself serves as the structural causal model required to answer such questions.\nErik Štrumbelj and Kononenko (2014) proposed additional improvements to the approximation algorithm using quasi-random and adaptive sampling. Since the primary contribution is an efficiency improvement in the approximation algorithm that relies on the same assumptions, we do not consider this a new method and the same considerations around quantifying the uncertainty of the estimates and interpreting the values correctly applies.\n\n\n4.3.1.3 KernelSHAP\nLundberg and Lee (2017) introduced a new method for estimating Shapley values defined using a conditional observational value function, referred to as KernelSHAP, that uses weighted linear regression to simulate feature removal using a joint marginal distribution.\n\\[\n\\hat{v}(S) = E[f(\\mathbf{x_s}, \\mathbf{X}_{\\bar{X}}]\n\\tag{4.5}\\]\nKernelSHAP, like SSV, is a sampling-based estimator that requires an independence assumption for the resulting values to be unbiased with respect to the defined value function. Therefore, the same considerations around quantifying uncertainty and interpreting the values as explanations apply.\nThe authors also proposed the term Shapley Additive Explanations (SHAP), which has subsequently been used to refer to different concepts in the literature. The original paper uses the term to refer to the collection of methods that define the target Shapley value in terms of a conditional observational value function. By this definition, all of the methods we have discussed thus far (SRV, SSV, and KernelSHAP) are SHAP methods. However, the term has also been used to refer to the class of additive feature attribution methods – methods whose attributions sum to the model’s output. All of the methods that fall under the SHAP umbrella are additive feature attribution methods, however, there are other methods (both Shapley-based and otherwise) that fall under this more general category. In other cases, SHAP is used to refer to the KernelSHAP estimation procedure or the SHAP python package, which includes multiple estimation procedures. Although it is counter to current practice, we recommend against using the term SHAP because of its multiple meanings and because, by the original definition, it is redundant with simply defining the value function associated with a Shapley-based method.\n\n\n4.3.1.4 Conditional KernelSHAP\nAas, Jullum, and Løland (2020)} developed an extension to KernelSHAP that estimates Shapley values corresponding to an observational conditional value function rather than an unconditional one. The authors propose four different ways to, more efficiently than SRV, approximate the required conditional distributions.\n\nMultivariate Gaussian distribution\nGaussian copula\nEmpirical conditional distribution\nCombination\n\nThe first option is best when the features are approximately normally distributed. When the features themselves are not normally distributed, but the dependence structure between them is well described by a normal distribution, the second option can be used. When neither the features nor their dependence structure can be described by a normal distribution, then the third option can be used. This option idea is similar to the smoothing-based approach suggested in Sundararajan and Najmi (2020) and involves taking an expectation over similar data points. The final option is to use one of the previous three alternatives depending on the number of features whose removal is simulated via conditioning. They note that using the empirical conditional distribution works well for a small number of conditioning variables, but one of the other three methods should be used otherwise.\nThe authors provide an empirical evaluation of the different alternatives using simulated data and find that for modest levels of correlation between features (\\(\\rho = 0.05\\)), all of their proposed extension methods provide better approximations than KernelSHAP. The authors use this result to claim that explanations arising from KernelSHAP can be very wrong when features are dependent. Although it is not explicitly stated, their implicit definition of correctness is about how closely the method used to estimate the Shapley value approximates the values as defined.\nIn our view, it is better to treat KernelSHAP and Conditional KernelSHAP as approximating different value functions rather than as better and worse approximations of the same value function. Both estimators yield Shapley values that can be used to provide correct model explanations provided they are used to address the correct types of target explanatory questions. For Conditional KernelSHAP, the resulting Shapley values form the basis for explanations that can address world-level associative explanatory questions only. Like KernelSHAP and SRV, Conditional KernelSHAP relies on sampling, so the same considerations around uncertainty quantification apply. Conditional KernelSHAP requires one additional consideration: the degree to which the distributional assumption associated with the approximation technique is valid.\n\n\n4.3.1.5 BShap\nSundararajan and Najmi (2020) were the first to explore Shapley-based explanations as a class of methods and discuss the apparent problems with an observational conditional value function. They show empirically how different methods that define the value function in the same way yield different Shapley values for a given feature, rendering the “uniqueness” result of the Shapley value practically meaningless. They solve this problem by proposing an alternative axiomatization that lends itself to a truly unique solution known as Baseline Shapley (BShap). This new axiomatization includes three new axioms (affine scale invariance, demand monotonicity, and proportionality) to the original three (dummy, symmetry, linearity) required to derive the original Shapley value.\nBShap simulates feature removal by replacing their values with the values from some fixed baseline (\\(\\mathbf{x}'\\)).\n\\[\n\\hat{v}(S) = f(\\mathbf{x}_S, \\mathbf{x}'_{\\bar{S}})\n\\]\nThe authors also introduce an extension to BShap called Random Baseline Shapley (RBShap) that takes an expectation over a collection of baseline values drawn according to some distribution \\(\\mathcal{D}\\).\n\\[\n\\hat{v}(S) = E_{\\mathcal{D}}[f(\\mathbf{x}_S, \\mathbf{x}'_{\\bar{S}}]\n\\]\nThey show that various Shapley-based methods can be subsumed under RBShap depending on the choice of \\(\\mathcal{D}\\) and for this reason, we treat RBShap as a unification approach rather than a separate Shapley-based method.\nThe authors were also the first in the Shapley-based local explanation literature to explore the theoretical and practical problems with defining the value function using an observational conditional value function. First, computing the necessary conditional expectations is computational challenging and fraught with additional complications. For example, using the training data to approximate the conditional distributions can be problematic due to sparsity. Conditioning on the “removed” features can be seen as filtering the training data down to those observations that agree with the instance being explained and then taking the expectation over the remaining observations. Especially when continuous variables are involved, the number of training data observations that match the instance to be explained is likely small. This sparsity problem means that the conditional expectation must, practically speaking, be estimated using some other approximation technique (e.g. one of the four alternatives noted in Aas, Jullum, and Løland (2020)). However, they note that these techniques either involve additional assumptions or computational complexity. Second, Sundararajan and Najmi (2020) showed that using an observational conditional expectation can lead to attributions that, under certain conditions, violate the Shapley axioms. In particular, they demonstrate that when features are correlated, a feature that is not functionally used by the model can receive a non-zero attribution, which violates the dummy axiom. As we saw earlier, the same argument was previously made in the Shapley-based global explanation literature, and is directly related to the indirect influence debate.\n\n\n\n4.3.2 Causal Methods\nIn contrast to the previous section, the following methods all explicitly incorporate causal reasoning into the explanation-generating process. The primary differentiator between these methods are the causal assumptions and auxiliary causal information required in order to generate the Shapley values.\n\n4.3.2.1 Causal Quantitative Input Influence (QII)\nDatta, Sen, and Zick (2016) introduced a family of measures for quantifying how much the inputs to a system influence the outputs. The ultimate goal is to use these measures to generate a “transparency report” for individuals subjected to an automated decision. Reminiscent of trends (both earlier and contemporaneously) in the relative importance literature, they were interested in providing measures of influence that take the correlation between inputs into account. However, they were the first, in both the global and local Shapley-based explanation literature, to frame this objective in explicitly causal terms with their causal QII method. As the emphasis on treating the model as an input-output system makes clear, they were concerned with model-level explanations. They define causal QII using an unconditional value function and simulate feature removal using the product of the marginal distributions of removed features.\n\\[\nv(S) = \\hat{v}(S) = E_{\\Pi_{i \\in C} p(\\mathbf{X}_i)}[f(\\mathbf{x}_S, \\mathbf{X}_{\\bar{S}})]\n\\]\nLike other Shapley-based methods that approximate an unconditional value function, explanations derived from causal QII are able to address model-level counterfactual (rung 3) questions. Although causal QII is limited to model-level explanations, it does not require any auxiliary information. There are two main differences between QII and other methods that approximate an unconditional value function. First, causal QII uses a different marginalization method than either KernelSHAP or SSV. Second, causal QII is motivated by taking an explicitly-causal perspective rather than assuming feature independence in order to simplify computing values associated with an observational conditional value function. It is this second difference that Janzing, Minorics, and Blöbaum (2019) are honing in on when they argue that the use of an unconditional value function, as implicitly argued by the creators of causal QII, is justified by a causal perspective.\n\n\n4.3.2.2 Asymmetric Shapley Values\nFrye, Rowat, and Feige (2020) introduced the first method, Asymmetric Shapley Values(ASV) that leverages auxiliary causal information to generate Shapley-based explanations. They define the value function using an observational conditional expectation (Equation 4.1}). Recognizing the difficulty of providing (and defending) a full graphical causal model, ASV requires only a partial causal ordering of the features. For example, given a set of features \\(X_1, X_2, ..., X_n\\), a practitioner may provide the ordering \\(\\{X_1, X_2\\}\\) indicating that \\(X_1\\) is a causal ancestor of \\(X_2\\). In Distal Asymmetric Shapley Values (d-ASV), this causal information is included by assigning zero weight to any permutations (Equation 3.2})for which \\(X_2\\) precedes \\(X_1\\). They argue that this aligns with explanations that attribute effects to root causes. Alternatively, Proximate Asymmetric Shapley Values (p-ASV) assigns zero weight to any permutations for which \\(X_1\\) precedes \\(X_2\\) such that attributions favor immediate causes. The non-uniform weighting of the permutations results in values that violate the symmetry axiom, which corresponds to a quasivalue from cooperative game theory.\n\n\n\n\n\n\n\n\nG\n\n\n\nX1\n\nX1\n\n\n\nX2\n\nX2\n\n\n\nX1-&gt;X2\n\n\n\n\n\nY\n\nY\n\n\n\nX1-&gt;Y\n\n\n\n\n\nX2-&gt;Y\n\n\n\n\n\n\n\n\nFigure 4.3: Alterative Graphical Causal Model with \\(\\{X_1, X_2\\}\\)\n\n\n\n\n\nOne problem with ASV is that a single causal ordering is consistent with multiple graphical causal models. For example, the ordering \\(\\{X_1, X_2\\}\\) is consistent with both (Figure 3.1 (a)) and (Figure 4.3). If the goal is to generate attributions that recover the causal relationship between \\(X_1\\) on \\(Y\\), these differences in the underlying graphical causal models are relevant (require different conditioning variables to satisfy the backdoor criterion), but are not accounted for in ASV. As a result, Heskes et al. (2020) note that even though ASV incorporates causal information, it can sometimes lead to improper (i.e. counter-intuitive) causal explanations.\nThe Shapley values generated by ASV do not map cleanly onto the types of explanatory questions as we have defined based on Pearl-style causality. ASV is not able to address model-level explanations because the estimated Shapley values do not align with an unconditional value function. When any partial causal ordering is provided, ASV is not able to address associative world-level questions because the weights assigned to different permutations lead to Shapley values that differ from those where a uniform weighting is applied. Similarly, these weights lead to values that also do not, in general, match values based on an interventional conditional value function Heskes et al. (2020). Although ASV-based Shapley values may provide valuable insights in other contexts, under our evaluation framework, they should not be used to provide model explanations.\n\n\n4.3.2.3 Causal Shapley Values\nHeskes et al. (2020) introduced an extension to ASV that leads to Shapley values that have a proper causal interpretation. Like Janzing, Minorics, and Blöbaum (2019), they define the target Shapley using an interventional conditional value function (Equation 4.2). However, their approach is more closely aligned with Frye, Rowat, and Feige (2020) in that they are interested in generating world-level explanations without requiring a full causal graph. Their key idea is to use a partial causal ordering of groups of features along with information about whether features within a group share a common ancestor or mutually-interact to generate a DAG of components, that is used in-lieu of a GCM (also a DAG). As a result, the practitioner does not need to provide a full GCM, but only a causal chain graph, which has a well-defined interventional formula that they derive using Pearl’s do-calculus.\nThe authors note that one of the main benefits of CSV is that the resulting explanations are able to differentiate between “direct” and “indirect” causal effects. These ideas are directly related to the indirect influence debate and our notion of levels of explanation. A direct causal effect is the causal effect of a feature on the model’s output. Shapley values based on an unconditional value function are only able to estimate these direct effects. As we noted previously, this means that features that are not functionally used by the model have zero direct effect. In contrast, a feature that is not functionally used may still have a non-zero indirect causal effect. We prefer to view these as providing different levels of explanation: a direct causal effect corresponds to a model-level explanation and an indirect causal effect corresponds to a world-level explanation.\nAnother contribution of their work is to clarify that whether a Shapley value is symmetric or asymmetric is a choice that can be made independently of how the value function is specified. While this may be obvious from examining Equation 3.2 and is well-known in the cooperative game theory literature (asymmetric values are known as quasivalues), it had not been surfaced previously in the Shapley-based model explanation literature.\nCSV has two practical limitations: it requires the explainer to provide substantial auxiliary causal information and requires approximating conditional distributions. The first is problematic as this type of auxiliary causal information simply may not be available because neither the explainer nor the explainee have sufficient domain expertise to provide the necessary information. The second limitation is not unique to CSV, but is still relevant to assessing the correctness of the resulting explanations. One of the alternatives for approximating the necessary conditional distributions proposed by Aas, Jullum, and Løland (2020) can be used. However, the considerations with applying one of these approaches that was discussed earlier still apply. When the necessary causal information is available and the required conditional distributions can be approximated, then CSV is a compelling option because it is able to generate explanations that address all types of model-level questions as well as associative and interventional world-level questions.\n\n\n4.3.2.4 Shapley Flow\nWang, Wiens, and Lundberg (2021) develop Shapley Flow(SF), which extends the set-based Shapley axioms to arbitrary graphs. Like ASV and CSV, they are interested in an approach that is able to generate world-level explanations, and like CSV, SF is able to generate both world and model-level explanations. One of the motivations for SF is that CSV divides the credit between a feature and it’s causal descendents, which they view as a counter-intuitive attribution policy. For example, consider a chain (see Figure 3.1 (a)) where their critique is that CSV splits the credit that should be assigned to \\(X_1\\) between \\(X_1\\) and \\(X_2\\). To avoid this issue, they use a rather idiosyncratic game formulation that requires the explainer to provide a structural causal model. The graphical causal model associated with the SCM contains nodes for each feature that is causally-related to the output, whether or not it is functionally used by the model. The edges in the GCM represent one of two things: a functional relationship between the feature and the model output, or a causal relationship between the features whether or not they are used by the model.\nShapley Flow departs from the typical game formulation, treating source-to-sink paths in the provided SCM as the players in the game and a partial ordering of these paths as the coalitions. Attributions are assigned to edges, whereas other methods assign credit to individual nodes. The attribution for an individual feature can be computed by summing the attributions of all incoming edges. The importance of each edge is computed by considering how much the model output changes when the edge is added. To simulate edge removal, they introduce the notion of active versus inactive edges. The foreground value is passed when the edge is active and the background value is passed when it is inactive. This foreground value is computed using the equation specified by the SCM. A background value can be a single value or a distribution of values. These background values are similar to BShap/RBShap Sundararajan and Najmi (2020) and single reference games and reference distributions from Merrick and Taly (2020). Using this setup, SF is capable of generating both model and world-level explanations.\nThe authors introduce the notion of a “boundary of explanation,” which is a more flexible way of framing the distinction between model and world-level explanations. To make things concrete, consider Figure 4.2. One boundary of explanation treats \\(\\hat{Y}\\) as the sink node and includes the edges \\(\\{(X_1, \\hat{Y}), (X_2, \\hat{Y})\\). This boundary leads to model-level explanations. Alternatively, the edges \\(\\{(\\tilde{X_1}, X_1), (\\tilde{X_2},\nX_2)\\}\\) lead to world-level explanations. One of the Shapley Flow axioms is boundary consistency, which ensures that the attribution for a given edge is the same across different explanation boundaries. Because of this axiom, they assign zero weight to certain orderings and is part of the reason for the idiosyncratic game formulation.\nIn principle, the SF framework is capable of generating explanations that address both model and world-level explanatory questions of all types (associative, interventional, and counterfactual). However, this power comes at the cost of requiring a structural causal model. As we saw earlier, an SCM is composed of a GCM as well as the functional (mathematical) equations governing the relationships between features. While challenging, it is conceivable that the explainer or explainee may be able to provide a densible GCM, that is, one that is consistent with the data as well as their domain expertise. However, the further practical problem of identifying the functional equations between features still remains. In their examples provided as part of the appendix, Wang, Wiens, and Lundberg (2021) approximate these functional relationships by training additional models. Each auxiliary model uses an endogenous feature from the GCM as the outcome and the parents of that feature as the inputs. The number of auxiliary models that must be trained is equal to the number of endogenous variables in the proposed GCM. Although SF is quite powerful, these practical considerations likely make the method infeasible for many use cases.\n\n\n4.3.2.5 Recursive Shapley Values\nSingal, Michailidis, and Ng (2021) introduced an alternative flow-based solution to the attribution problem using Shapley values called Recursive Shapley Values (RSV). RSV requires a graphical model as well as the functional relationships between variables; however, the authors note that the relationships do not need to be causal, allowing for them to capture arbitrary computation (e.g. a neural network).\nRSV shares some similarities with SF, but differs in how the game is formulated and how the Shapley values are computed. Like SF, RSV treats the provided graph as a messaging passing system (e.g. foreground and background values), assigns attributions to edges, and derives from a set of flow-based axioms that mirror the Shapley axioms. The players are the edges and coalitions are sets of edges, which more closely mirrors node-based approaches where features are players and sets of features constitute the coalitions. The final attributions are computed by combining the Shapley values from a sequence of games defined recursively in a top-down fashion starting with source nodes. In their view, this is a more natural way to formulate the game than the idiosyncratic way introduced by Wang, Wiens, and Lundberg (2021).\nRSV can be used to provide either model or world-level explanations. To generate model-level explanations, the explainer provides a graph where the model features are the source nodes and the model output is the sink node. World-level explanations can be generated by providing a structural causal model, which may include features not functionally used by the model. RSV suffers from the same practical limitations of SF, but is also potentially more computationally expensive. Because RSV is defined recursively, Shapley values for a sequence of games, rather than for a single game, is required.\n\n\n\n\nAas, Kjersti, Martin Jullum, and Anders Løland. 2020. “Explaining Individual Predictions When Features Are Dependent: More Accurate Approximations to Shapley Values.” arXiv:1903.10464 [Cs, Stat], February. http://arxiv.org/abs/1903.10464.\n\n\nAdler, Philip, Casey Falk, Sorelle A. Friedler, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, and Suresh Venkatasubramanian. 2016. “Auditing Black-Box Models for Indirect Influence.” arXiv:1602.07043 [Cs, Stat], November. http://arxiv.org/abs/1602.07043.\n\n\nBasu, Debraj. 2020. “On Shapley Credit Allocation for Interpretability.” arXiv:2012.05506 [Cs, Stat], December. http://arxiv.org/abs/2012.05506.\n\n\nCastro, Javier, Daniel Gómez, and Juan Tejada. 2009. “Polynomial Calculation of the Shapley Value Based on Sampling.” Computers & Operations Research 36 (5): 1726–30. https://doi.org/10.1016/j.cor.2008.04.004.\n\n\nCovert, Ian C. 2020. “Explaining by Removing: A Uniﬁed Framework for Model Explanation.” arXiv:2011.14878 [Cs], 90.\n\n\nDatta, Anupam, Shayak Sen, and Yair Zick. 2016. “Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems.” In 2016 IEEE Symposium on Security and Privacy (SP), 598–617. https://doi.org/10.1109/SP.2016.42.\n\n\nFeldman, Barry. 2005. “The Proportional Value of a Cooperative Game,” 30.\n\n\nFrye, Christopher, Colin Rowat, and Ilya Feige. 2020. “Asymmetric Shapley Values: Incorporating Causal Knowledge into Model-Agnostic Explainability.” arXiv:1910.06358 [Cs, Stat], October. http://arxiv.org/abs/1910.06358.\n\n\nGrömping, Ulrike. 2007. “Estimators of Relative Importance in Linear Regression Based on Variance Decomposition.” The American Statistician 61 (2): 139–47. https://www.jstor.org/stable/27643865.\n\n\nHeskes, Tom, Evi Sijben, Ioan Gabriel Bucur, and Tom Claassen. 2020. “Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models.” arXiv:2011.01625 [Cs], November. http://arxiv.org/abs/2011.01625.\n\n\nJanzing, Dominik, Lenon Minorics, and Patrick Blöbaum. 2019. “Feature Relevance Quantification in Explainable AI: A Causal Problem.” arXiv:1910.13413 [Cs, Stat], November. http://arxiv.org/abs/1910.13413.\n\n\nKruskal, William. 1987. “Relative Importance by Averaging Over Orderings.” The American Statistician 41 (1): 6–10. https://doi.org/10.2307/2684310.\n\n\nKumar, I. Elizabeth, Suresh Venkatasubramanian, Carlos Scheidegger, and Sorelle Friedler. 2020. “Problems with Shapley-Value-Based Explanations as Feature Importance Measures.” arXiv:2002.11097 [Cs, Stat], June. http://arxiv.org/abs/2002.11097.\n\n\nLindeman, Richard, Peter Merrenda, and Ruth Gold. 1980. Introduction to Bivariate and Multivariate Analysis. Glenview, IL.\n\n\nLipovetsky, Stan, and Michael Conklin. 2001. “Analysis of Regression in Game Theory Approach.” Applied Stochastic Models in Business and Industry 17 (4): 319–30. https://doi.org/10.1002/asmb.446.\n\n\nLundberg, Scott, and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” arXiv:1705.07874 [Cs, Stat], November. http://arxiv.org/abs/1705.07874.\n\n\nMerrick, Luke, and Ankur Taly. 2020. “The Explanation Game: Explaining Machine Learning Models Using Shapley Values.” arXiv:1909.08128 [Cs, Stat], June. http://arxiv.org/abs/1909.08128.\n\n\nOwen, Art B. 2014. “Sobol’ Indices and Shapley Value.” SIAM/ASA Journal on Uncertainty Quantification 2 (1): 245–51. https://doi.org/10.1137/130936233.\n\n\nOwen, Art B., and Clémentine Prieur. 2017. “On Shapley Value for Measuring Importance of Dependent Inputs.” SIAM/ASA Journal on Uncertainty Quantification 5 (1): 986–1002. https://doi.org/10.1137/16M1097717.\n\n\nSingal, Raghav, George Michailidis, and Hoiyi Ng. 2021. “Flow-Based Attribution in Graphical Models: A Recursive Shapley Approach.” {SSRN} {Scholarly} {Paper} ID 3845526. Rochester, NY: Social Science Research Network. https://doi.org/10.2139/ssrn.3845526.\n\n\nSong, Eunhye, Barry L. Nelson, and Jeremy Staum. 2016. “Shapley Effects for Global Sensitivity Analysis: Theory and Computation.” SIAM/ASA Journal on Uncertainty Quantification 4 (1): 1060–83. https://doi.org/10.1137/15M1048070.\n\n\nStrumbelj, Erik, and Igor Kononenko. 2010. “An Efficient Explanation of Individual Classifications Using Game Theory.” The Journal of Machine Learning Research 11 (March): 1–18.\n\n\nŠtrumbelj, E., I. Kononenko, and M. Robnik Šikonja. 2009. “Explaining Instance Classifications with Interactions of Subsets of Feature Values.” Data & Knowledge Engineering 68 (10): 886–904. https://doi.org/10.1016/j.datak.2009.01.004.\n\n\nŠtrumbelj, Erik, and Igor Kononenko. 2014. “Explaining Prediction Models and Individual Predictions with Feature Contributions.” Knowledge and Information Systems 41 (3): 647–65. https://doi.org/10.1007/s10115-013-0679-x.\n\n\nStufken, John. 1992. “On Hierarchical Partitioning.” The American Statistician 46 (1): 70–71. http://www.jstor.org/stable/2684415.\n\n\nSundararajan, Mukund, and Amir Najmi. 2020. “The Many Shapley Values for Model Explanation.” arXiv:1908.08474 [Cs, Econ], February. http://arxiv.org/abs/1908.08474.\n\n\nWang, Jiaxuan, Jenna Wiens, and Scott Lundberg. 2021. “Shapley Flow: A Graph-Based Approach to Interpreting Model Predictions.” arXiv:2010.14592 [Cs, Stat], February. http://arxiv.org/abs/2010.14592.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Shapley Explanations</span>"
    ]
  },
  {
    "objectID": "explanations.html#footnotes",
    "href": "explanations.html#footnotes",
    "title": "4  Shapley Explanations",
    "section": "",
    "text": "This notation largely mirrors Kumar et al. (2020) with slight differences to improve readability↩︎\nSee Grömping (2007) or a more detailed overview, which we summarize briefly here.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Shapley Explanations</span>"
    ]
  },
  {
    "objectID": "discussion.html",
    "href": "discussion.html",
    "title": "5  Discussion",
    "section": "",
    "text": "Now that we have covered the available methods in depth, we are in a position to revisit the second stage in our proposed framework, namely, how to select an appropriate Shapley explanation method. For a summary of our recommendations represented as a decision tree, see Figure 5.1.\n\n\n\n\n\n\nflowchart\n  A{Level}\n  A --&gt;|model| B[KernelSHAP]\n  A --&gt;|World| C{question}\n  C --&gt;|associative| D[Conditional KernelSHAP]\n  C --&gt;|interventional| E[CSV]\n  C --&gt;|counterfactual| F[RSV]\n\n\n\n\nFigure 5.1: Selecting A Shapley-based Method\n\n\n\n\n\nAs noted previously, the first step in the process is for the explainer to specify one or more target explanatory questions aligned with the explainees objectives. For each question, the explainer must then consider the following three questions:\n\nIs the explainee interested in a model or world-level explanation?\nIs the target explanatory question associative, interventional, or counterfactual?\nWhat degree of causal information is the explainee able to provide?\n\nTo answer the first question, we recommend engaging the explainee to understand where a hypothetical intervention, which captures the essence of their question, occurs. If a model-level explanation is required, then SSV, KernelSHAP, causal-QII, SF, and RSV are all appropriate choices. Due to the availability of open-source implementations, the relative simplicity of the method, and the fact that no auxiliary information must be provided by the explainee, we recommend KernelSHAP.\nThe second question is only relevant when a world-level explanation is required. For associative questions, we recommend either SRV or ConditionalKernelSHAP. When the number of features and the dataset are relatively small, we recommend SRV because it is an exact method, which limits the number of additional considerations that must be taken into account. For larger datasets or for models with a large number of features, we suggest Conditional KernelSHAP. Again, this recommendation is partially pragmatic due to the availability of an open-source implementation, which allows for different methods of estimating the required conditional distributions.\nFor interventional and counterfactual model-level explanations, the explainer must be able to provide auxiliary causal information in order for the question to be answerable. One way to identify the degree of causal information available is to engage the explainee in a process of generating a graphical causal model for their application. In the process, it may become clear that the explainee is either unable or unwilling to commit to providing such information. If the explainee can provide only information to develop the limited version of a GCM as required by CSV, then CSV is the clear choice. If a full GCM that is consistent with the available data can be elicited, then CSV, SF, and RSV are all appropriate. However, we recommend using CSV as it requires the least amount of auxiliary information and is more computationally efficient than the other two methods.\nIn exceptional cases, the explainee may be able to provide a full structural causal model. If this is the case, it may be worth working with the explainee to understand if machine learning is necessary to solve the original problem. If an explainee has sufficient domain expertise to generate an SCM, then it is unclear why the elicited SCM is not being used in lieu of a learned model. Provided the SCM is a valid description of the data generating process, it should be able to generate predictions at least as good as the model. Moreover, the SCM is sufficient for answering associative, interventional, and counterfactual questions that address a different level of explanation entirely. Instead of providing world-level model explanations, the SCM is sufficient to provide world-level explanations world explanations, and specifically, the aspect of the world previously predicted by the model.\nIt is critical to note that a Shapley explanation may not always be appropriate. This could occur for two reasons. First, the answers to the three aforementioned questions may not have a corresponding Shapley method. This could occur in a situation where the explainee is interested in a world-level counterfactual question, but cannot provide the necessary auxiliary causal information. At this point, the explainer may explore non-Shapley-based explanation methods, or go back to the explainee to identify a different explanatory question that is answerable using a Shapley-based method and aligned with their objectives. Alternatively, the answers to these questions may suggest that an entirely different modeling approach is more appropriate. Whether or not this is feasible largely depends on how the explanations will be used. For example, if explanations are required to satisfy a regulatory requirement for a deployed model, then a better alternative modeling approach is irrelevant.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "6  Conclusion",
    "section": "",
    "text": "In this work, we introduced a human-centric model explanation evaluation framework grounded in causal reasoning. Central to this framework is a process that starts with eliciting well-defined target explanatory questions. We then revisited the Shapley explanation literature in light of this framework, highlighting the importance of distinguishing between model and world-level explanations. Distinguishing between the two and recognizing the type of explanatory question that is of interest to the explainee allows for a more principled approach to selecting an appropriate Shapley-based method, or in some cases, to determining that a Shapley explanation is not appropriate.\nIn surveying the Shapley explanation literature, a number of potential avenues for future research emerged. First, there is an open question as to whether a causal perspective justifies non-Shapley explanation methods in particular circumstances. In particular, if a fully-specified SCM can be elicited, then should a machine learning approach for generating predictions have been taken in the first place? As a second example, if model-level explanations are desired, what are the benefits of using Shapley values instead of an individual conditional expectation plot (for local explanations) or a partial dependence plot (for global explanations)? Zhao and Hastie (2021) have shown that PDP and ICE plots have causal interpretations in situations where the complement set satisfies the backdoor criterion.\nThe debate over reference distributions within the Shapley explanation literature has parallels in the counterfactual explanation literature, highlighting the need for additional unification efforts within XAI. Specifically, Wachter, Mittelstadt, and Russell (2018) introduced the notion of counterfactual explanations to XAI and advocated for an unconditional distribution. Subsequent work has been divided on whether a marginal or interventional approach should be taken. These parallels suggest that a causal perspective may provide a useful foundation for such unification efforts. In fact, there is some work already moving in this direction (Viswanathan and Zick (2021), Beckers (2022)).\n\n\n\n\nBeckers, Sander. 2022. “Causal Explanations and XAI.” arXiv:2201.13169 [Cs], February. http://arxiv.org/abs/2201.13169.\n\n\nViswanathan, Vignesh, and Yair Zick. 2021. “Model Explanations via the Axiomatic Causal Lens.” arXiv:2109.03890 [Cs], September. http://arxiv.org/abs/2109.03890.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2018. “Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.” arXiv:1711.00399 [Cs], March. http://arxiv.org/abs/1711.00399.\n\n\nZhao, Qingyuan, and Trevor Hastie. 2021. “Causal Interpretations of Black-Box Models.” Journal of Business & Economic Statistics 39 (1): 272–81. https://doi.org/10.1080/07350015.2019.1624293.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Formulate Approximate Explain\nMerrick and Taly (2020) introduced the Formulate, Approximate Explain (FAE) framework characterized by the idea that the choice of how to estimate the value function must be chosen explicitly with a specific contrastive explanation in mind. They provide a critique, similar to Sundararajan and Najmi (2020) of methods (IME and SHAP) that define the value function using an observational conditional expectation. In particular, they provide a motivating example that shows how these methods can lead to attributions that violate both the dummy and symmetry Shapley value axioms.\nThe first step (formulate) in their framework is to generate a contrastive question, which they argue leads to a specific reference or distribution of references specific to the use-case. They show how different Shapley-based methods can be unified by considering the different choice of reference distribution used to simulate feature removal. Their notion of a reference distribution is roughly analogous to RBShap. They also introduce the idea of single reference games, which simulates feature absence by replacing values using a specific reference input (identical to BShap). In the second step, the Shapley value is computed in two steps: sampling references from the reference distribution and then approximating the average Shapley value for each feature over this set of single-reference games. This technique allows them to compute confidence intervals (assuming that the approximation method is unbiased), which are used to quantify the uncertainty in the Shapley value estimates as part of the final “explain” stage.",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#causal-reasoning-and-model-fairness",
    "href": "appendix.html#causal-reasoning-and-model-fairness",
    "title": "Appendix",
    "section": "Causal Reasoning and Model Fairness",
    "text": "Causal Reasoning and Model Fairness\nKilbertus et al. (2018) were the first to make the case that causal reasoning is required to assess model fairness1. In the Shapley value literature, Heskes et al. (2020) demonstrates how different causal structures, even if they yield the same observational distribution, can lead to different Shapley values. While this is true in general, they provide a concrete example that directly addresses the indirect influence debate. They compare different Shapley-based methods across four causal building blocks (chain, fork, confounder, and cycle) in a scenario where there are two features \\(X_1\\) and \\(X_2\\) and a model that depends only on \\(X_2\\) (as in our earlier example), demonstrating that many Shapley-based methods do not yield attributions with a proper world-causal explanation. For example, marginal SHAP methods are only able to estimate direct effects and therefore yield attributions without a proper world-causal explanation if the underlying structure is a chain (see Figure 3.1 (a)). On the other hand, observational conditional SHAP methods account for the dependence between features, but are unable to account for the fact that an intervention on the same variable in a fork and a confounder have different distributional implications and may result in attributions that do not have a proper world-causal interpretation. In our view, the marginal SHAP attributions have a proper model-causal interpretation even if interpreting them in a world-causal way is improper. Therefore, distinguishing between model and world causality is necessary in order to resolve the indirect influence debate through causal reasoning.\n\n\n\n\nHeskes, Tom, Evi Sijben, Ioan Gabriel Bucur, and Tom Claassen. 2020. “Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models.” arXiv:2011.01625 [Cs], November. http://arxiv.org/abs/2011.01625.\n\n\nKilbertus, Niki, Mateo Rojas-Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Schölkopf. 2018. “Avoiding Discrimination Through Causal Reasoning.” arXiv:1706.02744 [Cs, Stat], January. http://arxiv.org/abs/1706.02744.\n\n\nMerrick, Luke, and Ankur Taly. 2020. “The Explanation Game: Explaining Machine Learning Models Using Shapley Values.” arXiv:1909.08128 [Cs, Stat], June. http://arxiv.org/abs/1909.08128.\n\n\nSundararajan, Mukund, and Amir Najmi. 2020. “The Many Shapley Values for Model Explanation.” arXiv:1908.08474 [Cs, Econ], February. http://arxiv.org/abs/1908.08474.",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#footnotes",
    "href": "appendix.html#footnotes",
    "title": "Appendix",
    "section": "",
    "text": "More specifically, they are interested in unresolved discrimination↩︎",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aas, Kjersti, Martin Jullum, and Anders Løland. 2020. “Explaining\nIndividual Predictions When Features Are Dependent: More\nAccurate Approximations to Shapley Values.”\narXiv:1903.10464 [Cs, Stat], February. http://arxiv.org/abs/1903.10464.\n\n\nAdler, Philip, Casey Falk, Sorelle A. Friedler, Gabriel Rybeck, Carlos\nScheidegger, Brandon Smith, and Suresh Venkatasubramanian. 2016.\n“Auditing Black-Box Models for\nIndirect Influence.”\narXiv:1602.07043 [Cs, Stat], November. http://arxiv.org/abs/1602.07043.\n\n\nBasu, Debraj. 2020. “On Shapley Credit\nAllocation for Interpretability.”\narXiv:2012.05506 [Cs, Stat], December. http://arxiv.org/abs/2012.05506.\n\n\nBeckers, Sander. 2022. “Causal Explanations and\nXAI.” arXiv:2201.13169 [Cs], February. http://arxiv.org/abs/2201.13169.\n\n\nCastro, Javier, Daniel Gómez, and Juan Tejada. 2009. “Polynomial\nCalculation of the Shapley Value Based on Sampling.”\nComputers & Operations Research 36 (5): 1726–30. https://doi.org/10.1016/j.cor.2008.04.004.\n\n\nChen, Hugh, Joseph D. Janizek, Scott Lundberg, and Su-In Lee. 2020.\n“True to the Model or True to the\nData?” arXiv:2006.16234 [Cs, Stat], June.\nhttp://arxiv.org/abs/2006.16234.\n\n\nCovert, Ian C. 2020. “Explaining by Removing:\nA Uniﬁed Framework for\nModel Explanation.”\narXiv:2011.14878 [Cs], 90.\n\n\nDatta, Anupam, Shayak Sen, and Yair Zick. 2016. “Algorithmic\nTransparency via Quantitative\nInput Influence: Theory and\nExperiments with Learning\nSystems.” In 2016 IEEE\nSymposium on Security and Privacy\n(SP), 598–617. https://doi.org/10.1109/SP.2016.42.\n\n\nFeldman, Barry. 2005. “The Proportional\nValue of a Cooperative\nGame,” 30.\n\n\nFrye, Christopher, Colin Rowat, and Ilya Feige. 2020. “Asymmetric\nShapley Values: Incorporating Causal Knowledge into\nModel-Agnostic Explainability.” arXiv:1910.06358 [Cs,\nStat], October. http://arxiv.org/abs/1910.06358.\n\n\nGrömping, Ulrike. 2007. “Estimators of Relative\nImportance in Linear Regression\nBased on Variance\nDecomposition.” The American Statistician\n61 (2): 139–47. https://www.jstor.org/stable/27643865.\n\n\nHalpern, Joseph Y., and Judea Pearl. 2005a. “Causes and\nExplanations: A\nStructural-Model Approach.\nPart I: Causes.” The\nBritish Journal for the Philosophy of Science 56 (4): 843–87. http://www.jstor.org/stable/3541870.\n\n\n———. 2005b. “Causes and Explanations: A\nStructural-Model Approach.\nPart II: Explanations.”\nThe British Journal for the Philosophy of Science 56 (4):\n889–911. https://www.jstor.org/stable/3541871.\n\n\nHeskes, Tom, Evi Sijben, Ioan Gabriel Bucur, and Tom Claassen. 2020.\n“Causal Shapley Values:\nExploiting Causal Knowledge to\nExplain Individual Predictions of\nComplex Models.” arXiv:2011.01625\n[Cs], November. http://arxiv.org/abs/2011.01625.\n\n\nJanzing, Dominik, Lenon Minorics, and Patrick Blöbaum. 2019.\n“Feature Relevance Quantification in Explainable AI:\nA Causal Problem.” arXiv:1910.13413 [Cs,\nStat], November. http://arxiv.org/abs/1910.13413.\n\n\nKilbertus, Niki, Mateo Rojas-Carulla, Giambattista Parascandolo, Moritz\nHardt, Dominik Janzing, and Bernhard Schölkopf. 2018. “Avoiding\nDiscrimination Through Causal\nReasoning.” arXiv:1706.02744 [Cs, Stat],\nJanuary. http://arxiv.org/abs/1706.02744.\n\n\nKruskal, William. 1987. “Relative Importance by\nAveraging Over Orderings.”\nThe American Statistician 41 (1): 6–10. https://doi.org/10.2307/2684310.\n\n\nKumar, I. Elizabeth, Suresh Venkatasubramanian, Carlos Scheidegger, and\nSorelle Friedler. 2020. “Problems with\nShapley-Value-Based Explanations as Feature Importance\nMeasures.” arXiv:2002.11097 [Cs, Stat], June. http://arxiv.org/abs/2002.11097.\n\n\nLindeman, Richard, Peter Merrenda, and Ruth Gold. 1980. Introduction\nto Bivariate and Multivariate\nAnalysis. Glenview, IL.\n\n\nLipovetsky, Stan, and Michael Conklin. 2001. “Analysis of\nRegression in Game Theory Approach.” Applied Stochastic\nModels in Business and Industry 17 (4): 319–30. https://doi.org/10.1002/asmb.446.\n\n\nLundberg, Scott, and Su-In Lee. 2017. “A Unified\nApproach to Interpreting Model\nPredictions.” arXiv:1705.07874 [Cs, Stat],\nNovember. http://arxiv.org/abs/1705.07874.\n\n\nMerrick, Luke, and Ankur Taly. 2020. “The Explanation\nGame: Explaining Machine\nLearning Models Using\nShapley Values.” arXiv:1909.08128\n[Cs, Stat], June. http://arxiv.org/abs/1909.08128.\n\n\nMiller, Tim. 2018. “Explanation in Artificial\nIntelligence: Insights from the\nSocial Sciences.” arXiv:1706.07269\n[Cs], August. http://arxiv.org/abs/1706.07269.\n\n\nMittelstadt, Brent, Chris Russell, and Sandra Wachter. 2019.\n“Explaining Explanations in AI.”\nProceedings of the Conference on Fairness, Accountability, and\nTransparency, January, 279–88. https://doi.org/10.1145/3287560.3287574.\n\n\nOwen, Art B. 2014. “Sobol’ Indices and\nShapley Value.” SIAM/ASA Journal on\nUncertainty Quantification 2 (1): 245–51. https://doi.org/10.1137/130936233.\n\n\nOwen, Art B., and Clémentine Prieur. 2017. “On\nShapley Value for Measuring\nImportance of Dependent\nInputs.” SIAM/ASA Journal on Uncertainty\nQuantification 5 (1): 986–1002. https://doi.org/10.1137/16M1097717.\n\n\nPearl, Judea. 2009. Causality: Models,\nReasoning, and Inference. Second.\nCambridge University Press.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal\nInference in Statistics: A\nPrimer. Wiley.\n\n\nShapley, L. 1953. “A Value for n-Person\nGames.” In Contributions to the\nTheory of Games, 2:307–17. Princeton, NJ:\nPrinceton University Press.\n\n\nSingal, Raghav, George Michailidis, and Hoiyi Ng. 2021.\n“Flow-Based Attribution in\nGraphical Models: A\nRecursive Shapley\nApproach.” {SSRN} {Scholarly} {Paper} ID 3845526.\nRochester, NY: Social Science Research Network. https://doi.org/10.2139/ssrn.3845526.\n\n\nSong, Eunhye, Barry L. Nelson, and Jeremy Staum. 2016. “Shapley\nEffects for Global Sensitivity\nAnalysis: Theory and\nComputation.” SIAM/ASA Journal on Uncertainty\nQuantification 4 (1): 1060–83. https://doi.org/10.1137/15M1048070.\n\n\nStrumbelj, Erik, and Igor Kononenko. 2010. “An\nEfficient Explanation of\nIndividual Classifications Using\nGame Theory.” The Journal of\nMachine Learning Research 11 (March): 1–18.\n\n\nŠtrumbelj, E., I. Kononenko, and M. Robnik Šikonja. 2009.\n“Explaining Instance Classifications with Interactions of Subsets\nof Feature Values.” Data & Knowledge Engineering 68\n(10): 886–904. https://doi.org/10.1016/j.datak.2009.01.004.\n\n\nŠtrumbelj, Erik, and Igor Kononenko. 2014. “Explaining Prediction\nModels and Individual Predictions with Feature Contributions.”\nKnowledge and Information Systems 41 (3): 647–65. https://doi.org/10.1007/s10115-013-0679-x.\n\n\nStufken, John. 1992. “On Hierarchical\nPartitioning.” The American Statistician 46\n(1): 70–71. http://www.jstor.org/stable/2684415.\n\n\nSundararajan, Mukund, and Amir Najmi. 2020. “The Many\nShapley Values for Model Explanation.”\narXiv:1908.08474 [Cs, Econ], February. http://arxiv.org/abs/1908.08474.\n\n\nViswanathan, Vignesh, and Yair Zick. 2021. “Model\nExplanations via the Axiomatic\nCausal Lens.” arXiv:2109.03890\n[Cs], September. http://arxiv.org/abs/2109.03890.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2018.\n“Counterfactual Explanations Without\nOpening the Black Box:\nAutomated Decisions and the\nGDPR.” arXiv:1711.00399 [Cs], March. http://arxiv.org/abs/1711.00399.\n\n\nWang, Jiaxuan, Jenna Wiens, and Scott Lundberg. 2021. “Shapley\nFlow: A Graph-Based\nApproach to Interpreting Model\nPredictions.” arXiv:2010.14592 [Cs, Stat],\nFebruary. http://arxiv.org/abs/2010.14592.\n\n\nZhao, Qingyuan, and Trevor Hastie. 2021. “Causal\nInterpretations of Black-Box\nModels.” Journal of Business & Economic\nStatistics 39 (1): 272–81. https://doi.org/10.1080/07350015.2019.1624293.",
    "crumbs": [
      "References"
    ]
  }
]